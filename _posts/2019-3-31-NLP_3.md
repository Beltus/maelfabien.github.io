---
published: true
title: Word2Vec Embedding
collection: ml
layout: single
author_profile: false
read_time: true
categories: [machinelearning]
excerpt : "Natural Language Processing"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
    teaser : "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
toc_sticky: true
sidebar:
    nav: sidebar-sample
---

In order to analyze text and run algorithms on it, we need to embed the text. The notion of embedding simply means that we'll conver the input text into a set of numerical vectors that can be used into algorithms. In this article, we'll focus on Word2Vec, a state of the art embedding method

For the sake of clarity, we'll call a document a simple text, and each document is made of words, which we'll call terms.

<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

{% highlight matlab %}
{% endhighlight %}

# I. Why should we embed words ?

Embedding, as stated above, is used to create feature vectors from words. The idea is to encode their meaning and allow us to calculate a similarity score for any pair of words for example.

What can we do with embedded corpus ?
- determine similarity scores between documents for a search engine
- identify topics of the documents
- build a recommender system to suggest other movies based on the synopsis
- machine translation to identify that "Au revoir" and "Goodbye" actually mean the same thing

# II. How does Word2Vec work ?

Word2Vec works pretty much as an auto-encoder. We will train on one side a neural network to perform a certain task on one side, and on the other side to undo it to get back to the original result.

## 1. First part of the model

First of all, we need to define the list of vocabulary. The task we'll teach our model to do is to learn the probability that two words are "nearby" within a text. For each "target" word among our vocabulary, we do that by :
- defining a window size
- picking randomly a word within the window size around the target word
- counting the number of times they appear together
- applying softmax activation to create probabilities

The output probabilities are a measure of how likely it is that these 2 words appear together.

We give the network as an entry the pairs of words. To add randomness into the process, the window size in the training process is chosen randomly, between 1 and the specified window size.

![image](https://maelfabien.github.io/assets/images/nlp_3.jpgs)

If we have 5'000 words of vocabulary and we fit our model on it, the output of the netword should be a 5'000 $$ \times $$ 5'000 matrix, and within each position the probability that these words appear together.



> **Conclusion** : I hope this quick introduction to Bag-Of-Words in NLP was helpful. Don't hesitate to drop a comment if you have a comment.
