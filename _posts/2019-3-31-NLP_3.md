---
published: true
title: Bag-Of-Words Embedding (BOW)
collection: ml
layout: single
author_profile: true
read_time: true
categories: [machinelearning]
excerpt : "Natural Language Processing"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
    teaser : "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
toc_sticky: true
---

In order to analyze text and run algorithms on it, we need to embed the text. The notion of embedding simply means that we'll conver the input text into a set of numerical vectors that can be used into algorithms. In this article, we'll focus on Word2Vec, a state of the art embedding method

For the sake of clarity, we'll call a document a simple text, and each document is made of words, which we'll call terms.

<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

{% highlight matlab %}
{% endhighlight %}

# I. Why should we embed words ?

Embedding, as stated above, is used to create feature vectors from words. The idea is to encode their meaning and allow us to calculate a similarity score for any pair of words for example.

What can we do with embedded corpus ?
- determine similarity scores between documents for a search engine
- identify topics of the documents
- build a recommender system to suggest other movies based on the synopsis
- machine translation to identify that "Au revoir" and "Goodbye" actually mean the same thing

# II. What is Bag-Of-Words ?

## 1. Term Frequency

When we use Bag-Of-Words approaches, we apply a simple *word embedding* technique. Technically speaking, we take our whole corpus that has been preprocessed, and create a giant matrix :
- the columns correspond to all the vocabulary that has ever been used with all the documents we have at our disposal
- the lines correspond to each of the document
- the value at each position corresponds to the number of occurence of a given token within a given document

Bag-Of-Words (BOW) can be illustrated the following way :

![image](https://maelfabien.github.io/assets/images/nlp_2.png)

The number we fill the matrix with are simply the raw count of the tokens in each document. This is called the ***term frequency*** (TF) approach.

$$ tf_{t,d} = f_{t,d} $$

where :
- the term or token is denoted $$ t $$
- the document is denoted $$ d $$
- and $$ f $$ is the raw count

This approach is infact only half of the story ! What are the limitations implied by this model ?
- the more frequent a word, the more importance we attach to it within each document which is logic. However, this can be problematic since common words, like cat or dog in our example, do not bring much information about the document. In other words, a word that appear

## 2. Term Frequency Inverse Document Frequency (TF-IDF)

For the reasons mentioned above, the TF-IDF methods were quite popular for a long time, before more advanced techniques like Word2Vec or Universal Sentence Encoder.

In TF-IDF, instead of filling the BOW matrix with the raw count, we simply fill it with the term frequency multiplied by the inverse document frequency. It is intended to reflect how important a word is to a document in a collection or corpus.

$$ tf_idf{t,d} = f_{t,d} \times \frac {1} { f_{t} }  $$

# III. Implementation in Python

Let's now implement this in Python. 

```python

```

# IV. Limits of BOW methods

The reason why BOW methods are not so popular these days are the following :
- the voculabulary size might get very, very (very) large, and handling a sparse matrix with over 100'000 features is not so cool. If you want to control it, you should set a maximum document length, or a maximum vocabulary length. Both imply large biases.
- the order of the words in the sentence does not matter, which is a major limitation

For example, the sentences : "The cat’s food was eaten by the dog in a few seconds." does not have the same meaning at all than "The dog's food was eaten by the cat in a few seconds.".

In the next article, we'll see more evolved techniques like Word2Vec which perform much better and are currently close to state of the art.

> **Conclusion** : I hope this quick introduction to Bag-Of-Words in NLP was helpful. Don't hesitate to drop a comment if you have a comment.
