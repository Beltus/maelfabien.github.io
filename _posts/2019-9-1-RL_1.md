---
published: false
title: Introduction to Reinforcement Learning
collection: rl
layout: single
author_profile: false
read_time: true
categories: [RL]
excerpt : "Advanced AI"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
    teaser: "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
toc_sticky: true
sidebar:
    nav: sidebar-sample
---

I was recently recommended to take a look at David Silver's (from DeepMind) YouTube series on Reinforcement Learning. The whole course (10 videos) can be found [here](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ).

In this series, I am going to :
- summarize the important notions taught in each course
- add examples and code
- illustrate some concepts with schemas
- do a Deep RL project
- go a bit further by exploring Twin Delayed DDPG and other SOTA models

# Introduction to RL

Reinforcement Learning is at the intersection of many fields of science :
- Machine Learning
- Reward System
- Classical/Operant conditioning
- Bounded Rationality
- Operations Research
- Optimal Control

How is RL different from other ML paraddigms ?
- There is no supervisor, only a reward signal
- Feedback is delayed, not instantaneous
- Time matters, since we consider sequences
- The agent's actions affect the data it sees

Where is RL applied ?
- manage an investment portfolio
- make a robot walk
- control a power station
- play games (Atari, Go...)

## The RL learning problem

A **reward** $$ R_t $$ is a feedback value. In indicates how well the agent is doing at step $$ t $$. The job of the agent is to maximize the cumulative reward. 

> Reward Hypothesis : All goals can be described by the maximisation of expected cumulative reward.




![image](https://maelfabien.github.io/assets/images/xception.jpg)
