---
published: true
title: Text Preprocessing
collection: st
layout: single
author_profile: false
read_time: true
categories: [machinelearning]
excerpt : "Natural Language Processing"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
    teaser : "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
toc_sticky: true
sidebar:
    nav: sidebar-sample
---

One of the main challenge when you're dealing with text is to build an efficient preprocessing pipeline.

<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

{% highlight matlab %}
{% endhighlight %}

# I. What is preprocessing ?

Preprocessing in Natural Language Processing (NLP) is the process by which we try to "standardize" the text we want to analyze.

A challenge that arises pretty quickly when you try to build an efficient preprocessing NLP pipeline is the diversity of the texts you might deal with :
- tweets that would be highly informal
- cover letters from candidates in a HR company
- Slack messages within a team
- Even code sometime if you try to analyze Github comments for example

The diversity makes the whole thing tricky. Usually, a given pipeline is developped for a certain kind of text. The pipeline should give us a "clean" text version.

Another challenge that arises when dealing with text preprocessing is the language. English language remains quite simple to preprocess. German or french use for example much more special characters like "é, à, ö, ï". 

You might now wonder what are the main steps of preprocessing ?
- A first step is to remove words that are made of special characters (if needed in your case) : `@,#, /,!.\'+-= `
- In English, some words are short versions of actuals words, e.g "I'm" for "I am". To treat them as separate words, you'll need to split them.
- We then would like to remove specific syntax linked to our text extraction, e.g "\n" everytime there is a new line
- Remove the stop words, which are mainstream words like "the, I, would"...
- Once this step is done, we are ready to tokenize the text, i.e split by word
- To make sure that the words "Shoe" and "shoe" are later understood as the same, lower case the tokens
- Lemmatize the tokens to extract the "root" of each word.

The process can be illustrated the following way :

![image](https://maelfabien.github.io/assets/images/nlp_1.jpg)

# II. More into details

Tokenization consists in splitting large chunks of tesxt into sentences, and sentences into a list of single words, also called tokens. This step, also referred to as segmentation or lexical analysis, is necessary in order to perform further processing.

Once tokens are obtained, it is common to standardize them. Standardization for instance includes lowercasing tokens or deleting some punctuation characters that are not crucial to the understanding of the text. The removing of stopwords in order to retain only words with meaning is also an important standardization step as it allows to get rid of words that are too common like ‘a’, ‘the’ or ‘an’. One way to see this task is to see it as a way to put all words on an equal footing.

Then, there are methods available in order to replace words by their grammatical root : the goal of both stemming and lemmatization is to reduce derivationally related forms of a word to a common base form. Stemming eliminates affixes (suffixes, prefixes, infixes, circumfixes) from a word in order to obtain a word stem while lemmatization is able to capture canonical forms based on a word’s lemma. Families of derivationally related words with similar meanings, such as ‘am’, ‘are’, ‘is’ would then be replace by the word ‘be’.

Finally, in the context of word sense disambiguation, part-of-speech tagging is used in order to mark up words in a corpus as corresponding to a particular part of speech, based on both its definition and its context. This part is not illustrated in the pipeline above, but is typically really useful to improve the accuracy of the lemmatization process. 

After all these preprocessibng steps, we need to convert the text files into numerical feature vectors. This step, called vectorization, consists in converting a corpus of documents into a matrix of token counts, the number of features being equal to the vocabulary size. This conversion allows to perform visualization (word frequencies, word clouds…) or classification for instance. 

# III. In Python

We are now ready to implement this in Python ! First, import some packages :

```python
from nltk import wordpunct_tokenize, WordNetLemmatizer, sent_tokenize, pos_tag
from nltk.corpus import stopwords as sw, wordnet as wn
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import string 
```

We'll be using NLTK as our reference package for this tasks.

## 1. Preprocessing per document

We can define the preprocessing pipeline that will process each document as a single entity and apply the preprocessing on it :


```python
def preprocess(document, max_features = 150, max_sentence_len = 300):
    """
    Returns a normalized, lemmatized list of tokens 
    from a list of document, applying word/punctuation
    tokenization, and finally part of speech tagging. It uses the part of
    speech tags to look up the lemma in WordNet, and returns the lowercase  
    version of all the words, removing stopwords and punctuation.
    """

    def lemmatize(token, tag):
        """
        Converts the tag to a WordNet POS tag, then uses that   
        tag to perform an accurate WordNet lemmatization.
        """
        tag = {
            'N': wn.NOUN,
            'V': wn.VERB,
            'R': wn.ADV,
            'J': wn.ADJ
        }.get(tag[0], wn.NOUN)

    return WordNetLemmatizer().lemmatize(token, tag)

    def vectorize(doc, max_features, max_sentence_len):
        """
        Converts a document into a sequence of indices of length max_sentence_len retaining only max_features unique words
        """
        tokenizer = Tokenizer(num_words=max_features)   
        tokenizer.fit_on_texts(doc)
        doc = tokenizer.texts_to_sequences(doc)
        doc_pad = pad_sequences(doc, padding = 'pre', truncating = 'pre', maxlen = max_sentence_len)
        return np.squeeze(doc_pad), tokenizer.word_index

    cleaned_document = []
    vocab = []

    # For each document inside the corpus
    for sent in document:

        sent = re.sub(r"[^A-Za-z0-9^,!.\/'+-=]", " ", sent)
        sent = re.sub(r"what's", "what is ", sent)
        sent = re.sub(r"\'", " ", sent)
        sent = re.sub(r"@", " ", sent)
        sent = re.sub(r"\'ve", " have ", sent)
        sent = re.sub(r"can't", "cannot ", sent)
        sent = re.sub(r"n't", " not ", sent)
        sent = re.sub(r"i'm", "i am ", sent)
        sent = re.sub(r"\'re", " are ", sent)
        sent = re.sub(r"\'d", " would ", sent)
        sent = re.sub(r"\'ll", " will ", sent)
        sent = re.sub(r"(\d+)(k)", r"\g<1>000", sent)
        sent = sent.replace("\n", " ")

        lemmatized_tokens = []

        # Break the sentence into part of speech tagged tokens
        for token, tag in pos_tag(wordpunct_tokenize(sent)):

            # Apply preprocessing to the tokens
            token = token.lower()
            token = token.strip()
            token = token.strip('_')
            token = token.strip('*')

            # If punctuation ignore token and continue
            if all(char in set(string.punctuation) for char in token): #token in set(sw.words('english')) or 
                continue

            # Lemmatize the token
            lemma = lemmatize(token, tag)
            lemmatized_tokens.append(lemma)
            vocab.append(lemma)

        cleaned_document.append(lemmatized_tokens)

    vocab = sorted(list(set(vocab)))

    return cleaned_document, vocab
```

```python
df, vocab = preprocess(list(df))
```

## 2. Preprocessing per sentence

If your aim is to do an embedding per sentence, without taking into account the structure of the documents within the corpus, then, this pipeline might be more appropriate :


```python
def preprocess(document, max_features = 150, max_sentence_len = 300):
    """
    Returns a normalized, lemmatized list of tokens from a document by
    applying segmentation (breaking into sentences), then word/punctuation
    tokenization, and finally part of speech tagging. It uses the part of
    speech tags to look up the lemma in WordNet, and returns the lowercase
    version of all the words, removing stopwords and punctuation.
    """

    def lemmatize(token, tag):
        """
        Converts the tag to a WordNet POS tag, then uses that
        tag to perform an accurate WordNet lemmatization.
        """
        tag = {
            'N': wn.NOUN,
            'V': wn.VERB,
            'R': wn.ADV,
            'J': wn.ADJ
        }.get(tag[0], wn.NOUN)

        return WordNetLemmatizer().lemmatize(token, tag)

    def vectorize(doc, max_features, max_sentence_len):
        """
        Converts a document into a sequence of indices of length max_sentence_len retaining only max_features unique words
        """
        tokenizer = Tokenizer(num_words=max_features)
        tokenizer.fit_on_texts(doc)
        doc = tokenizer.texts_to_sequences(doc)
        doc_pad = pad_sequences(doc, padding = 'pre', truncating = 'pre', maxlen = max_sentence_len)
        return np.squeeze(doc_pad), tokenizer.word_index

    cleaned_document = []
    vocab = []

    # Clean the text using a few regular expressions
    document = re.sub(r"[^A-Za-z0-9^,!.\/'+-=]", " ", document)
    document = re.sub(r"what's", "what is ", document)
    document = re.sub(r"\'", " ", document)
    document = re.sub(r"@", " ", document)
    document = re.sub(r"\'ve", " have ", document)
    document = re.sub(r"can't", "cannot ", document)
    document = re.sub(r"n't", " not ", document)
    document = re.sub(r"i'm", "i am ", document)
    document = re.sub(r"\'re", " are ", document)
    document = re.sub(r"\'d", " would ", document)
    document = re.sub(r"\'ll", " will ", document)
    document = re.sub(r"(\d+)(k)", r"\g<1>000", document)
    document = document.replace("\n", " ")


    # Break the document into sentences
    for sent in sent_tokenize(document):
        lemmatized_tokens = []

        # Break the sentence into part of speech tagged tokens
        for token, tag in pos_tag(wordpunct_tokenize(sent)):

            # Apply preprocessing to the tokens
            token = token.lower()
            token = token.strip()
            token = token.strip('_')
            token = token.strip('*')

            # If punctuation ignore token and continue
            if all(char in set(string.punctuation) for char in token): #token in set(sw.words('english')) or 
                continue

            # Lemmatize the token
            lemma = lemmatize(token, tag)
            lemmatized_tokens.append(lemma)
            vocab.append(lemma)

        cleaned_document.append(lemmatized_tokens)

    vocab = sorted(list(set(vocab)))    
    return cleaned_document, vocab
```

And apply this function on a string version of the whole corpus :

```python
df, vocab = preprocess(str(list(df)))
```

> **Conclusion** : I hope this quick introduction to preprocessing in NLP was helpful. Don't hesitate to drop a comment if you have a comment.
