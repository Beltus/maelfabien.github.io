---
published: true
title: Introduction to Data Viz
collection: ml
layout: single
author_profile: false
read_time: true
categories: [machinelearning]
excerpt : "Data Viz"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
    teaser : "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
toc_sticky: true
sidebar:
    nav: sidebar-sample
---

<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

{% highlight matlab %}
{% endhighlight %}

In this article, we'll introduce the field of Data Visualization. I'm trying to bring a clear summary of the different courses I have had and the resources I have read. I am listing the resources I am making reference to at the end of this article.

There are many things you should already be familiar with, but I think it's a great exercise to formalize it. Hopefully, you'll also learn some new stuffs !

# What is Data Viz ?

Let's kick this with a definition of Data Visualization :

> Visualization is a cognitive process that allows to form a mental image to gain insights, discover, make decisions and explain. Data Visualization is the use of computer-supported, interactive visual representations of data to amplify cognition.

## Basis of Data Viz

There are 3 main subfields of Data Viz :
- Scientific visualization, whose role is to model real world phenomena 
- Information visualization, whose role is to map a more abstract concept into 2D or 3D for decision making and analysis purposes
- Visual analytics, which is a the frontier of Data mining and Machine Learning

There are 3 main types of variables :
- Qualitative (Nominal or ordinal)
- Quantitative
- Metadata which is descriptive information about the data

The are cognitive benefits of information visualization, since it can amplify cognition in several ways :
– Increasing memory and processing resources available
– Reducing search for information – Enhancing the recognition of patterns – Enabling perceptual inference operations 
– Using perceptual attention mechanisms for monitoring 
– Encoding information in a manipulable medium 

The value of information visualization can be computed using the information of the number of users visualizing the data, the frequency and the time spared at each time.

The key attributes to implement or not when developing a visualization tool are :
- the scalability to a large number of data
- the interactivity in order to offer multiple views to a user

## Why use Data Viz ?

The volume of data produced yearly is growing exponentially. From 800 Exabytes in 2008 to 900 Zetabytes in 2017 (so large it's even hard to count the number of 0s : 900000000000000000000000 bytes). How do we make sense of such a large amount of data ? How can we make this data understandable and exploitable ?

We use Data Viz to help people rapidly narrow in from a large space and find parts of the data to study more carefully. Visualization and visual contents have been chosen since among the human senses, vision has the highest band with, with over 100Mb/s theoretically. Ears only allow the processing of 100 bytes/second on the other hand. 

Why is vision so effective ?
- *Preattentive processing* : some visual features can be perceived very rapidly and accurately by our low-level visual system (e.g identifying a red dot in the middile or grey dots)
- *Gastalt theory* : the visual systems understands an image using proximity, similarity, continuity, symmetry, close and relative size features.

We use the viz for better decision making, better anomaly detection... Overall, we want the end user to be better and more effective at his task. Computers allow simply to map a large volume of data and to update the visualization as often as needed. 

## When to use Data Viz ?

There is a number of conditions for information browsing to be useful :
- When there is a good underlying structure so that items close to one another can be inferred to be similar
- When users are unfamiliar with a collection’s contents
- When users have limited understanding of how a system is organized and prefer a less cognitively loaded method of exploration
- When users have difficulty verbalizing the underlying information need
- When information is easier to recognize than describe 

# Data mapping

How do we map data to a *representation* ? 
- We first define a space, usually using axis
- Then we define the marks we'll be using, which are the things that occur in the space (points, stars, dots...)
- Then the graphical properties of marks (size, position, orientation, color, texture...)

Our aim should be to increase the use of space, encode data, and make the graph efficient. There are overall only 5 main categories of graphs :

- *Data tables*, like Excel Sheets for example

![image](https://maelfabien.github.io/assets/images/france19.png)

- *Graphs on rails* or planes (visually, we can see a 1D plot as a single rail, a 2D plot as a combination of rails, a Pie chart as a single rail folded...)

![image](https://maelfabien.github.io/assets/images/france20.png)

- *Geospatial maps*, which is the mapping of the latitude and longitude on a 2D plane, in which we can add some information (features on a map, size of components...)

![image](https://maelfabien.github.io/assets/images/france21.png)

- *Network diagrams*, whose role is to display the relation between items. In such diagrams 

![image](https://maelfabien.github.io/assets/images/graph_9.png)

- *Symboles and conceptual images*, which can be any logo whose aim is to present an information (e.g the PEGI 12 logo on a video game)

One of the issues that arises is when we need to represent hypervariate data. How to do this ?
- Represent the data on a data table. This is a high dimensional mapping on a 2D plane. This is however not very efficient.
- Reduce the dimension using PCA or t-SNE for example, but this implies loosing the meaning behind the clusters for example
- Plot a scatterplot matrix to identify the correlation between individual dimensions, but this does not combine the different dimensions, and we can easily get too many graphs to look at.

![image](https://maelfabien.github.io/assets/images/france22.png)

- Encode dimensions in features of the markers : e.g the size of the marker, the shape, the color... This can be really efficient, but there is a limit to the number of dimensions we can map, and it quickly gets impossible to read. (Check Chernoff Faces for a fun example of how to map many dimensions on human-looking faces.)
- Use parallel coordinates : we encode variable along a horizontal row. Parallel coordinates are the best way to represent high dimensional data. Vertical lines specifies different values that variable can take. Data points are represented as polyline. Different variable can have values taking quite different ranges. We might need to scale it. We also need to reorder the dimensions to have the most similar next to one another.

![image](https://maelfabien.github.io/assets/images/france23.png)

# Famous Tools

Along with `matplotlib, pandas, seaborn`..., there are some really good tools that are used for Data Visualization : 
- Tableau, for drag&drop UX, and great dashboards
- Altair, for a programmatic tool similar to Tableau, in Python
- D3.js for interactive JS graphs
- ...

# Recommended resources

The key resources I have been working with in this field are the following :
- Visualization Analysis and Design by Tamara Munzner
- [The Value of Information Visualization](https://www.win.tue.nl/~vanwijk/infovis_springer.pdf)


You can download the dataset right [here](https://maelfabien.github.io/assets/files/france.csv).

> **Conclusion** : That's it ! I hope this introduction to Altair was interesing. I really love to use this on my Data Viz projects. Feel free to leave a comment if you have one.
