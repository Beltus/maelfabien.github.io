---
published: true
title: Hidden Markov Model (HMM)
collection: ml
layout: single
author_profile: false
read_time: true
categories: [machinelearning]
excerpt : "Markov Processes and HMM"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
    teaser : "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
toc_sticky: true
---

So far, we covered Markov Chains. Now, we'll dive into more complex models : Hidden Markov Models.

<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

{% highlight matlab %}
{% endhighlight %}

Hidden Markov Models (HMM) are widely used for :
- speech recognition
- writing recognition
- object or face detection
- part-of-speech tagging and other NLP tasks...

I recommend to check the introduction made by Luis Serrano on HMM on [YouTube](https://www.youtube.com/watch?v=kqSzLo9fenk)

# I. Context

We will be focusing on Part-of-Speech (PoS) tagging. Part-of-speech tagging is the process by which we are able to tag a given word as being a noun, pronoun, verb, adverb... 

![image](https://maelfabien.github.io/assets/images/hmm_12.png)

PoS can for example be used for Text to Speech conversion or Word sense disambiguation.

![image](https://maelfabien.github.io/assets/images/hmm_13.png)

In this specific case, the same word `bear` has completely different meanings, and the corresponding PoS is therefore different. 

Let's consider the following scenario. In your office, there are 2 colleagues that talk a lot. You know they either talk about **Work** or **Animals**. Since they look cool, you'd like to join them. But you're too far to understand the whole conversation, and you only get some words of the sentence

Before joining the conversation, in order not to sound too weird, you'd like to guess whether he talks about **Work** or **Animals**. For example, here is the kind of sentence your friends might be pronouncing :

![image](https://maelfabien.github.io/assets/images/hmm_14.png)

## Emission probabilities

You only hear distinctively the words **python** or **bear**, and try to guess the context of the sentence. Since your friends are Python developers, when they talk about work, they talk about Python 80% of the time.

![image](https://maelfabien.github.io/assets/images/hmm_15.png)

These probabilities are called the Emission probabilities.

## Transition probabilities

You listen to their conversations and keep trying to understand the subject every minute. There is some sort of coherence in the conversation of your friends. Indeed, if one hour they talk about work, there is a lower probability that the next minute they talk about animals.

We can define what we call the Hidden Markov Model for this situation :

![image](https://maelfabien.github.io/assets/images/hmm_16.png)

The probabilities to change the topic of the conversation or not are called the transition probabilities.

- The words you understand are called the *observations*, since you observe them. Logic.
- The subject they talk about is called the hidden state, since you can't observe it

## How can we find the transition probabilities ?

They are based on observations we have made. We can suppose that after carefully listening, every minute, we manage to understand the topic they were talking about. This does not give us the full information on the topic they are currently talking about though.

You have 15 observations, taken over the last 15 minutes, **W** denotes Work and **H**  Holidays.

![image](https://maelfabien.github.io/assets/images/hmm_17.png)

We notice that in 2 cases out of 5, the topic Work lead to the topic Holidays, which explains the transition probability in the graph above.

## How can we find the emission probabilities ?

Well, since we have observations on the topic they were discussion, and we observe the words that were used during the discussion, we can define estimates of the emission probabilities :

![image](https://maelfabien.github.io/assets/images/hmm_18.png)

## What is the probability for each topic at a random minute ?

Suppose that you were not listening to they conversation anymore. 


> **Conclusion** : I hope this was clear enough ! HMMs are really interesing topics, so don't hesitate to drop a comment !
