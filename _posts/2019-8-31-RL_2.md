---
published: true
title: Markov Decision Process
collection: rl
layout: single
author_profile: false
read_time: true
categories: [RL]
excerpt : "Advanced AI"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
    teaser: "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
toc_sticky: true
sidebar:
    nav: sidebar-sample
---

*David Silver's YouTube series on Reinforcement Learning, Episode 2*. 

<script type="text/javascript" async
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

# Markov Decision Process (MDP)

A Markov Decision Process descrbes an environment for reinforcement learning. The environment is fully observable. In MDPs, the current state completely characterises the process.

The **Markov Property** states the following:

> A state $$ S_t $$ is **Markov** if and only if $$ P(S_{t+1} \mid S_t) = P(S_{t+1} \mid S_1, ..., S_t) $$

The transition between a state $$ s $$ and the next state $$ s' $$ is characterized by a **transition probability**. It is defined by :

$$ P_{ss'} = P(S_{t+1} = s' \mid S_t = s) $$

We can characterize a **state transition matrix** $$ P $$, describing all transition probabilities from all states $$ s $$ to all successor states $$ s' $$, where each row of the matrix sums to 1.

$$
P = 
\begin{bmatrix} 
P_{11} &  \cdots & P_{1n} \\
\cdots &  \cdots & \cdots \\
P_{n1} &  \cdots & P_{nn} \\
\end{bmatrix}
$$

A **Markov Process** is a memoryless random process. It is a sequence of randdom states $$ S_1, S_2, \cdots $$ with the Markov Property.

A Markov Process, also known as Markov Chain, is a tuple $$ (S,P) $$, where :
- $$ S $$ is a finite set of states
- $$ P $$ is a state transition probability matrix such that $$ P_{ss'} = P(S_{t+1} = s' \mid S_t = s) $$

We can represent a Markov Decision Process schematically the following way :

![image](https://maelfabien.github.io/assets/images/hmm_4.jpg)

**Samples** describe chains that take different states. For example, it could be :
- 1, 1, 2, 1, 2, 3, 4, 3, Exit
- 1, 2, 3, Exit
- ...

The transition matrix corresponding to this problem is :

![image](https://maelfabien.github.io/assets/images/hmm_5.jpg)

# Markov Reward Process

A Markov Reward is a Markov Chain a value function. A **Markov Reward Process** is a tuple $$ (S, P, R, \gamma) $$ where :
- $$ R $$ is a reward function $$ R_s = E(R_{t+1} \mid S_t = s) $$
- $$ \gamma $$ is a discount factor between 0 and 1

We can therefore attach a reward to each state in the following graph :

![image](https://maelfabien.github.io/assets/images/rl_7.png)

Then, the **Return** is the total discounte reward from time-step $$ t $$ :

$$ G_t = R_{t+1} + \gamma R_{t+2} + \cdots = \sum_{k=0} \gamma^k R_{t+k+1} $$

Just like in Finance, we compute the present value of future rewards. This represents the fact that we prefer to get reward now instead of getting it in the future.
- if $$ \gamma $$ is close to 0, we have a "myopic" evaluation where almost only the present matters
- if $$ \gamma $$ is close to 1, we have a "far-sighted" evaluation

But why do we use a discount factor ?
- there is uncertainty in the future, and our model is not perfect
- it avoids infinite returns in cyclical Markov Processes
- animals and humans have a preference for immediate reward


