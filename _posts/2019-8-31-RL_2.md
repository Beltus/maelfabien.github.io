---
published: true
title: Markov Decision Process
collection: rl
layout: single
author_profile: false
read_time: true
categories: [RL]
excerpt : "Advanced AI"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
    teaser: "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
toc_sticky: true
sidebar:
    nav: sidebar-sample
---

*David Silver's YouTube series on Reinforcement Learning, Episode 2*. 

<script type="text/javascript" async
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

A **Markov Decision Process** descrbes an environment for reinforcement learning. The environment is fully observable. In MDPs, the current state completely characterises the process.

# Markov Process (MP)

The **Markov Property** states the following:

> A state $$ S_t $$ is **Markov** if and only if $$ P(S_{t+1} \mid S_t) = P(S_{t+1} \mid S_1, ..., S_t) $$

The transition between a state $$ s $$ and the next state $$ s' $$ is characterized by a **transition probability**. It is defined by :

$$ P_{ss'} = P(S_{t+1} = s' \mid S_t = s) $$

We can characterize a **state transition matrix** $$ P $$, describing all transition probabilities from all states $$ s $$ to all successor states $$ s' $$, where each row of the matrix sums to 1.

$$
P = 
\begin{bmatrix} 
P_{11} &  \cdots & P_{1n} \\
\cdots &  \cdots & \cdots \\
P_{n1} &  \cdots & P_{nn} \\
\end{bmatrix}
$$

A **Markov Process** is a memoryless random process. It is a sequence of randdom states $$ S_1, S_2, \cdots $$ with the Markov Property.

A Markov Process, also known as Markov Chain, is a tuple $$ (S,P) $$, where :
- $$ S $$ is a finite set of states
- $$ P $$ is a state transition probability matrix such that $$ P_{ss'} = P(S_{t+1} = s' \mid S_t = s) $$

We can represent a Markov Decision Process schematically the following way :

![image](https://maelfabien.github.io/assets/images/hmm_4.jpg)

**Samples** describe chains that take different states. For example, it could be :
- 1, 1, 2, 1, 2, 3, 4, 3, Exit
- 1, 2, 3, Exit
- ...

The transition matrix corresponding to this problem is :

![image](https://maelfabien.github.io/assets/images/hmm_5.jpg)

# Markov Reward Process (MRP)

## Markov Reward

A Markov Reward is a Markov Chain a value function. A **Markov Reward Process** is a tuple $$ (S, P, R, \gamma) $$ where :
- $$ R $$ is a reward function $$ R_s = E(R_{t+1} \mid S_t = s) $$
- $$ \gamma $$ is a discount factor between 0 and 1
- all other components are the same as before

We can therefore attach a reward to each state in the following graph :

![image](https://maelfabien.github.io/assets/images/rl_7.png)

Then, the **Return** is the total discounte reward from time-step $$ t $$ :

$$ G_t = R_{t+1} + \gamma R_{t+2} + \cdots = \sum_{k=0} \gamma^k R_{t+k+1} $$

Just like in Finance, we compute the present value of future rewards. This represents the fact that we prefer to get reward now instead of getting it in the future.
- if $$ \gamma $$ is close to 0, we have a "myopic" evaluation where almost only the present matters
- if $$ \gamma $$ is close to 1, we have a "far-sighted" evaluation

A simple return for the sequence 1-1-2-3-Exit and with $$ \gamma = 0.8 $$ would be :

$$ G_1 = (-10) + (-10) * 0.8 + (-5) * 0.8^2 + (-1) * 0.8^3 $$

But why do we use a discount factor ?
- there is uncertainty in the future, and our model is not perfect
- it avoids infinite returns in cyclical Markov Processes
- animals and humans have a preference for immediate reward

## Bellman Equation

The **value function** $$ v(s) $$ gives the long-term value of a state $$ s $$. It reflects the expected return when we are in a given state :

$$ v(s) = E(G_t \mid S_t = s) $$

The value function can be decomposed in two parts :
- the immediate reward $$ R_{t+1} $$
- the discounted value of the successor rate $$ \gamma v(S_{t+1}) $$

This is the **Bellman Equation** for MRP :

$$ v(s) = E(G_t \mid S_t = s) = E(R_{t+1} + \gamma R_{t+2} + \cdots \mid S_t = s) $$

$$ = E(R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + \cdots) \mid S_t = s $$

$$ = E(R_{t+1} + \gamma G_{t+1} \mid S_t = s)  = E(R_{t+1} + \gamma v(S_{t+1}) \mid S_t = s)  $$

$$ = R_s + \gamma \sum_{s' \in S} P_{ss'} v(s') $$

If we consider that $$ \gamma $$ is equal to 1, we can compute the value function at state 2 in our previous example :

![image](https://maelfabien.github.io/assets/images/rl_8.png)

We can summarize the Bellman equation is a matrix form :

$$ v = R + \gamma P v $$

$$
\begin{bmatrix} 
v(1) \\
\cdots \\
v(n) \\
\end{bmatrix}

= 

\begin{bmatrix} 
R_1 \\
\cdots \\
R_n \\
\end{bmatrix}

+ \gamma

\begin{bmatrix} 
P_{11} &  \cdots & P_{1n} \\
\cdots &  \cdots & \cdots \\
P_{n1} &  \cdots & P_{nn} \\
\end{bmatrix}

\begin{bmatrix} 
v(1) \\
\cdots \\
v(n) \\
\end{bmatrix}

$$

We can solve this equation as a simple linear equation :

$$ v = R + \gamma P v $$

$$ (1 - \gamma P) v = R $$

$$ v = (1 - \gamma P)^{-1} R $$

However, solving this equation this way has a computational complexity of $$ O(n^3) $$ for $$ n $$ states since it contains a matrix inversion step. There are several ways to compute it faster, and we'll develop those solutions later on.

# Markov Decision Process (MDP)

So far, we have not seen the action component. Markov Decision Process (MDP) is a Markov Reward  Process with decisions. As defined at the beginning of the article, it is an environment in which all states are Markov.

A **Markov Decision Process** is a tuple of the form : $$ (S, A, P, R, \gamma) $$ where :
- $$ A $$ is a finite set of actions
- $$ P $$ the state probability matrix is now modified : $$ P_{ss'}^a = P(S_{t+1} = s' \mid S_t = s, A_t = a) $$
- $$ R $$ the reward function is now modified : $$ R_s^a = E(R_{t+1} \mid S_t = s, A_t = a) $$
- all other components are the same as before

We now have more control on the actions we can take :

![image](https://maelfabien.github.io/assets/images/rl_9.png)

There might stil be some states in which we cannot take action and are subject to the transition probabilities, but in other states, we have an action choice to make. This is the reality of an agent, and we needd to maximize the reward and find the best path to reach the final state.

But what does it mean to actually **make a decision** ?

## Policies


