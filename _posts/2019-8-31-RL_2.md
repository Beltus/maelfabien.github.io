---
published: true
title: Markov Decision Process
collection: rl
layout: single
author_profile: false
read_time: true
categories: [RL]
excerpt : "Advanced AI"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
    teaser: "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
toc_sticky: true
sidebar:
    nav: sidebar-sample
---

*David Silver's YouTube series on Reinforcement Learning, Episode 2*. 

<script type="text/javascript" async
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

# Markov Decision Process (MDP)

A Markov Decision Process descrbes an environment for reinforcement learning. The environment is fully observable. In MDPs, the current state completely characterises the process.

The Markov Property states the following:

> A state $$ S_t $$ is **Markov** if and only if $$ P(S_{t+1} \mid S_t) = P(S_{t+1} \mid S_1, ..., S_t) $$

The transition between a state $$ s $$ and the next state $$ s' $$ is characterized by a **transition probability**. It is defined by :

$$ P_{ss'} = P(S_{t+1} = s' \mid S_t = s) $$

We can characterize a **state transition matrix** $$ P $$, describing all transition probabilities from all states $$ s $$ to all successor states $$ s' $$, where each row of the matrix sums to 1.

$$
P = 
\begin{bmatrix} 
P_{11} &  \cdots & P_{1n} \\
\cdots &  \cdots & \cdots \\
P_{n1} &  \cdots & P_{nn} \\
\end{bmatrix}
$$






![image](https://maelfabien.github.io/assets/images/rl_4.png)
