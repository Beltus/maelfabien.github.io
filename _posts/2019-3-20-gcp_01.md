---
published: true
title: Introduction to Google Cloud Platform - Week 1 
collection: bgd
layout: single
author_profile: false
read_time: true
categories: [bigdata]
excerpt : "Road to Google Cloud Platform Certification"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
    teaser: "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
toc_sticky: true
sidebar:
    nav: sidebar-sample
---

The following series of articles is based on the Data Engineering with Google Cloud Platform specializaton on Coursera. This is a 1 months program, that required about 16h of work per week.

# Introduction to GCP

## History and context

How were computing resources managed over time?
- 1980s: Server on-premises. You own everything, and you manage it.
- 2000s: Data Centers. Rent the space, but pay and manage the hardware. No direct physical access to the computers.
- Now: First Generation Cloud with Virtualized Data Centers. You rent hardware and space, still controlling and configuring virtual machines. Pay only for what you provision.
- Next: Managed Services. Completely elastic storage, processing, ML. Pay for what you use.

Google has built one of the most powerful infrastructures on the planet. On every 5 CPUs being produced worldwide, Google buys one. Google has over 100 points of presence that use private fiber, on all continents. The idea of having so many edge locations is to allow users to access the resources without having to go across the globe, but simply get a cached version of that resource from the edge location. 

The main software released by Google in Data Engineering for data processing are :
- Google File System, Colossus
- MapReduce, Dremel, Flume
- BigTable
- Tensorflow
- ...

![image](https://maelfabien.github.io/assets/images/gcp_1.jpg)

## What is GCP ?

Most of the time, companies want to move from their local environment to Google Cloud Platform. Therefore, to move to the Cloud, the steps are :
- change where you compute
- improve scalability and reliability, scaling up
- change how you compute, transforming your business

Overall, GCP allows to :
- spend less on ops and administration
- incorporate real-time data into apps and architectures
- apply machine learning broadly and easily 
- become a truly data-driven company

There are four fundamental aspects of Google's core infrastructure and a top layer of products and services that you will interact with most often. 

![image](https://maelfabien.github.io/assets/images/gcp_3.png)

# Compute Power for Analytic and ML Workloads

8 Google products serve more than 1 billion users. For a simple Google Photo stabillization video service, we talk about a billion data points (several Mega pixels for each image of a video). Over 1.2 billion of photos are uploaded every day on Google Photo (13 PetaBytes). On YouTube, it's more than 400 hours every minute (1 PetaByte).

A phone's hardware is not sophisticated enough to train a ML model of that size. Google pre-trains models in its data centers and deploys trained models on the devices. Google open sources pre-trained models as building blocks :

![image](https://maelfabien.github.io/assets/images/gcp_4.png)

Jeff Dean estimates that if everyone used Voice Search for 3 minutes, Google would need to double its infrastructure. Moore's Law is not catching up the rate, and we reach physical limits. One of the ways to overcome this issue is to develop specialized hardware for a given task. This is what Google is doing with Tensor Processing Units (TPUs), that are specialized for Machine Learning and Deep Learning with more memory and faster processing. For example, eBay uses TPUs for their infrastructure and allows them to speed their processes by a factor of 10.

![image](https://maelfabien.github.io/assets/images/gcp_5.png)

Google optimized the data center cooling energy by 40% and improved the power usage effectiveness (PUE) by 15% recently.

# Compute Power for Analytic and ML Workloads

So far, we used storage to host one CSV file, an image and a HTML file. But how should we store all the data generated ? In the Storage Bucket too !

![image](https://maelfabien.github.io/assets/images/gcp_22.png)

The data needs to be separated from the Compute Engine. There are 2 ways to create buckets :

![image](https://maelfabien.github.io/assets/images/gcp_23.png)

For big-data analytics, we usually choose regional storage or multi-regional storage for geo-redundancy. Nearline and Coldline are respectively used for monthly and yearly access to data. The resource access hierarchy is the following in GCP :

![image](https://maelfabien.github.io/assets/images/gcp_24.png)

An organization has several folders, each of which contains projects (with unique names), and each project has services attached to it. Organization and folders are however not mandatory, but useful to define rules that apply to a whole organization. 

To move a file to a bucket, the gs_util can be used the following way :

![image](https://maelfabien.github.io/assets/images/gcp_25.png)

# Build on Google's Global Network

![image](https://maelfabien.github.io/assets/images/gcp_26.png)

Google has laid thousands of miles of fiber optic cable that crosses oceans with repeaters to amplify optical signals. Google's data centers around the world are interconnected by this private Google network, which by some publicly available estimates, carries as much as 40 percent of the world's internet traffic everyday. 

Thanks to the speed of the data center networks at Google, they are now able to split computation and storage, and no longer have both on a same machine. The speedd reached 1 Petabyte/s of total bisection bandwidth. This basically means that you can perform computation on data located elsewhere.

You need to serve out the results of your analytics and predictions, perhaps to users who are all around the world. This is where Edge points of presence come in. 

![image](https://maelfabien.github.io/assets/images/gcp_27.png)

The network, Google's Network, interconnects with the public Internet at more than 90 internet exchanges and more than 100 points of presence worldwide. When an Internet user sends traffic to a Google resource, Google responds to the user's request from an Edge network location that will provide the lowest delay or latency. Google's Edge caching network places content close to end-users to minimize latency. Your applications in GCP, like your machine learning models, can take advantage of this Edge network too.

# Security: On-premise vs Cloud-native

![image](https://maelfabien.github.io/assets/images/gcp_28.png)

Thanks to its scale, Google can manage a lot of security layers that would be almost impossible to manage (at that level) for an on-premise service. The only part left to secure for the user is the data access policy, but Google provides tools such as IIM to define policies. Communications to Google are encrypted in transit and offer multiple layers of security. Stored data is automatically encrypted at rest and distributed for availability and reliability.

In Big Query, data are key encrypted, and keys are themselves key-encrypted.

# Evolution of Google Cloud Big Data Tools

![image](https://maelfabien.github.io/assets/images/gcp_3.png)

Google invented new data processing methods as it grew :
- Google File System to handle large amount of data
- MapReduce to distribute computations over clusters
- Recording and retrieving millions of rows with BigTable
- Dremel to automate MapReduce
- ...

![image](https://maelfabien.github.io/assets/images/gcp_30.png)


> **Conclusion **: This is the end of the Introduction To Google Cloud Platform module, part of the week 1 of the GCP Specialization.
