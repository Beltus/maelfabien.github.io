---
published: true
title: Markov Processes
collection: ml
layout: single
author_profile: false
read_time: true
categories: [machinelearning]
excerpt : "Markov Processes and HMM"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
    teaser : "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
toc_sticky: true
---

In this series of articles, we'll focus on Markov Models, where an when they should be used, and extensions such as Hidden Markov Models.

<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

{% highlight matlab %}
{% endhighlight %}

We'll cover into further details when to use Markov Models. For the moment, one should remember that Markov Models, and especially Hidden Markov Models (HMM) are used for :
- speech recognition
- writing recognition
- object or face detection
- and several NLP tasks ...

# I. Stochastic model


![image](https://maelfabien.github.io/assets/images/hmm_2.png)

Let's start by defining what a stochastic model is. It is essentially a discrete time process $$ q_1, q_2, ... $$ indexed at times $$ 1, 2, ... $$ whose "states" are observed. The states simply correspond to the actual values of the process, usually defined by a finite space : $$ S = 1, ... Q $$.

The process starts at an initial state $$ q_1 $$. Then, according to transition probabilities, we move between the states. We can compute the probability of a sequence of states using Bayes Rule :

$$ P(q_1, q_2, ... q_T) = P(q_T \mid q_1, q_2, ... q_{T-1}) \times P(q_1, q_2, ... q_{T-1} ) $$

$$ = P(q_T \mid q_1, q_2, ... q_{T-1}) \times P(q_{T-1} \mid q_1, q_2, ... q_{T-2}) \times P(q_1, q_2, ... q_{T-2} ) = ... $$

$$ = P(q_1) P(q_2 \mid q_1) P(q_3 \mid q_1, q_2) ... P(q_T \mid q_1, q_2, ... q_{T-1}) $$

In order to characterize the model, we need :
- the initial probability $$ P(q_1) $$
- all the transition probabilities

As you might guess, this is complex to achieve since we need to know a lot of parameters.

# II. Discrete Time Markov Chain Models (DTMC)

## What is a Markov Chain ?

![image](https://maelfabien.github.io/assets/images/hmm_3.png)

Discrete Time Markov Chain (DTMC) are time and event discrete stochastic process. Markov Chains rely on the Markov Property that there is a limited dependence within the process :

$$ P(q_t \mid q_1, ..., q_{t-1}) = P(q_t \mid q_{t-k}, ..., q_{t-k-1}) $$ where $$ k = 1 $$ or $$ 2 $$.

When $$ k = 1 $$, $$ P(q_t \mid q_1, ..., q_{t-1}) = P(q_t \mid q_{t-1}) $$ 

$$ P(q_1, q_2, ..., q_T) = P(q_1) P(q_2 \mid q_1) ... P(q_T \mid q_{T-1}) $$

Let's illustrate this ! Consider a simple maze in which a mouse is trapped. We will denote $$ q_t $$ the position of the maze in which the mouse stands after $$ t $$ steps. We will assume that the mouse does not have memory of the steps it took within the maze. It simply goes to the position randomly, following the probability written next to each move.

![image](https://maelfabien.github.io/assets/images/hmm_4.png)

## Transition probabilities and matrix

A Discrete Time Markov chain is said to be homogenuous if its transition probabilities do not depend on the the $$ t $$ :

$$ P(q_t = j \mid q_{t-1} = i) = P(q_{t+k} = j \mid q_{t+k-1} = i) = a_{ij} $$

We can summarize the process is a transition matrix denoted $$ A = [a_{ij}], i \in 1...Q, j \in 1...Q $$. A transition matrix is stochastic if :
- all entries are non negative
- each line sums to 1

In our example, the transition matrix would be :

![image](https://maelfabien.github.io/assets/images/hmm_5.png)

Note that if $$ A $$ is stochastic, then $$ A^n $$ is stochastic.

## States

There are several ways to describe a state. Let $$ p_{ii} $$ be the probability of returning to state $$ i $$ after leaving $$ i $$ :
- a state $$ i $$ is transient if $$ p_{ii} < 1 $$ 
- a state $$ i $$ is recurrent if $$ p_{ii} = 1 $$ 
- a state $$ i $$ is absorbing if $$ a_{ii} = 1 $$ 

Therefore, a state is positive recurrent if the average time before return to this same state, denoted $$ T_{ii} $$ is finite.

A DTMC is irreducible if a state $$ j $$ can be reached in a finite number of steps from any other state $$ i $$. An irreducible DTMC is in fact a strongly connected graph.

A state in a discrete-time Markov chain is periodic if the chain can return to the state only at multiples of some integer larger than 1. 

For example :

![image](https://maelfabien.github.io/assets/images/hmm_6.png)

Otherwise, it is called aperiodic. A state with a self loop, i.e $$ a_{ii} $$ is always aperiodic. 

## Sojourn time

Let $$ T_i $$ be the time spent in state $$ i $$ before jumping to other states.

Then, $$ T_i $$ follows a geometric distribution :

$$ P(T_i = n) = {a_{ii}}^{n-1}(1-a_{ii}) $$

The expected average time spent is $$ E(T) = \frac {1}{a_{ii}} $$

## m-step transition

The probability of going from $$ i $$ to $$ j $$ in $$ m $$ steps is denoted by :

$$ {a_{ij}}^(m) = P(X_{n+m} = j \mid X_n = i) = P(X_m = j \mid X_0 = i) $$

We can see $$ {a_{22}}^(4) $$ as the probability for the mouse to still be in position 2 at time $$ t = 4 $$. Therefore, the probability of going from $$ i $$ to $$ j $$ in exactly $$ n $$ steps is given by $$ {f_{ij}}^(n) $$ where :

$$ f_{ij} = a_{ij} + \sum_{kâ‰ jj} a_{ik} f_{kj} $$

## Probability distribution of states

Let $$ \pi_i(n) $$ be the probability of being in state $$ i $$ at time $$ n $$ : $$ \pi_i(n) = P(X_n = i) $$. 

Then, $$ \pi(n) = [ \pi_1(n), \pi_2(n), ...] $$ is the vector of probability distribution which depends on :
- the initial transition matrix A
- the initial distribution $$ \pi(0) $$

Notice that : $$ \pi(n+1) = \pi(n) A $$ and recursively : $$ \pi(n) = \pi(0) A^n $$

For an irreducible / aperiodic DTMC, the distribution $$ \pi(n) $$ converges to a limit vector $$ \pi $$ which is independent of $$ \pi(0) $$ and is the unique solution of :

$$ \pi = \pi P $$ 

And :

$$ \sum_i \pi_i = 1 $$

$$ \pi_i $$ is also called stationary probabilities, steady state or equilibrium distribution.

## Generating a sequence

When we want to generate a sequence, we start from an initial state $$ q_1 = 1 $$ for example. The general idea is that 


> **Conclusion** : I hope this quick introduction to Bag-Of-Words in NLP was helpful. Don't hesitate to drop a comment if you have a comment.
