---
published: true
title: Linear Discriminant Analysis
collection: st
layout: single
author_profile: false
read_time: true
categories: [machinelearning]
excerpt : "Supervised Learning Algorithms"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
    teaser: "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
toc_sticky: true
---

Linear Discriminant Analysis is a generative model for classification. It is a generalization of Fisher's linear discriminant. LDA works on continuous variables. If the classification task includes categorical variables, the equivalent technique is called the discriminant correspondance analysis.

The goal of Linear Discriminant Analysis is to project the features in higher dimension space onto a lower dimensional space to both reduce the dimension of the problem and achieve classification.

<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

## Key ideas

- Generative Model that tries to estimate $$ P (X = x \mid Y = 1) $$ and $$ P (X = x \mid Y = -1) $$
- Used for classification
- Requires continuous variables
- Relies on normality assumption for $$ P(X \mid Y = 1) $$ and $$ P(X \mid Y = 0) $$
- Requires homoscedasticity and full rank covariances

## Theory

Let's suppose that we have 2 classes. Using LDA, we would like to find the two underlying marginal distributions. 

We'll define the distributions of the 2 classes :
- $$ G = L(X \mid Y=1) $$ distribution of the class 1
- $$ H = L(X \mid Y=-1) $$ distribution of the class -1

To understand the intuition behind LDA, we can define a likelihood ratio :
$$ \phi(X) = \frac { \delta G} { \delta H} (x) = \frac {P(X = x \mid Y = 1)} {P(X = x \mid Y = -1)} $$

Using Bayes' theorem :

$$ \phi(X) = \frac {P(Y = 1 \mid X = x) \frac {P(X=x)} {P(Y=1)}} {P(Y = -1 \mid X = x) \frac {P(X=x)} {P(Y=-1)}} $$

$$ \phi(X) = \frac { \frac { P(Y = 1 \mid X = x) } { P(Y=1) } } { \frac { P(Y = -1 \mid X = x) } { P(Y=-1) } } $$

We can re-define $$ P(Y=1) $$ as $$ p $$ and $$ P(Y=1 \mid X = x) $$ as the prior probability $$ \eta(x) $$.

$$ \phi(X) = \frac {1-p} {p} \frac {\eta(x)} {1-\eta(x)} $$

We can easily isolate the prior probability $$ \eta(x) $$ :

$$ \eta(x) = \frac { p \phi(x) } {(1-p) + p \phi(x)} $$ 

## Hypothesis

The LDA relies on some strong hypothesis which we'll explicit now.

### Gaussian marginal distributions 

- $$ G = N(\mu_+, \sigma_+) $$
- $$ H = N(\mu_-, \sigma_-) $$

where :

$$ N(\mu, \sigma^2) = \frac {1} {\sqrt {2  \pi \sigma^2}}  e^{ \frac {-1} {2 \sigma^2} {(x-\mu)^2}} $$

![image](https://maelfabien.github.io/assets/images/lda_1.png)

### Homoscedasticity

LDA should be used when the covariance matrices are equal among the 2 classes :

$$ \sigma_+ = \sigma_- = \sigma $$

## Computation

How do we find the parameters of the model ? How does the lerning process work ?

$$ \eta(x) = \frac { e^{ ( \frac {-1} {2} ( x - \mu_+ )^T \sigma^{-1} (x-\mu_+) ) } } {e^{ ( \frac {-1} {2} (x - \mu_-)^T \sigma^{-1} (x- \mu_- ) ) } } $$

$$ = e^{ ( \frac {-1} {2} (x-\mu_+)^T \sigma^{-1} (x-\mu_+) + \frac {-1} {2} (x-\mu_-)^T \sigma^{-1} (x-\mu_-) ) } $$

$$ = e^{ (x^T \sigma^{-1} {\mu_+}^T - \frac {1} {2} \mu_+ \sigma_{-1} \mu_- - x^T \sigma^{-1} \mu_- + \frac {1} {2} {\mu_-}^T \sigma_{-1} \mu_- ) } $$

If $$ \eta(x) > \frac {1} {2} $$, then $$ \phi(x) ≥ \frac {1-p} {p} $$ . This means that :

$$ x^T \sigma^{-1} (\mu_+ - \mu_-) + \frac {1} {2} ( {\mu_+}^T \sigma_{-1} \mu_- {\mu_+}^T \sigma^{-1} \mu_+) ≥ log \frac {p} {1-p} $$

Which can be re-written as :

$$ \alpha + \beta^T x ≥ 0 $$

Where :

$$ \beta = \aigma^{-1}(\mu_+ - \mu_-) $$

$$ \alpha = \frac {1} {2} ({\mu_-}^t \sigma^{-1} \mu_- - {\mu_+}^t \sigma^{-1} \mu_+) - \log \frac {p} {1-p} $$

The question now becomes : How can we estimate $$ \alpha $$ and $$ \beta $$ ?

$$ \hat{p} = \frac { \sum_i I_{y_i = 1} } { n} = \frac {n_+} {n} $$

$$ \hat{\mu_+} = \frac { 1} {n_+} \sum_{Y_i = 1} X_i $$

$$ \hat{\mu_-} = \frac {1} {n_-} \sum_{Y_i = -1} X_i $$

$$ \hat{\sigma} = \frac {n_+} {n} \hat{\sigma_+} + frac {n_-} {n} \hat{\sigma_-} $$

We then simply replace those values to find $$ \hat{\alpha} $$ and $$ \hat{\beta} $$.

> **Conclusion** : I hope this introduction to LDA was helful. Let me know in the comments if you have any question.
