---
published: true
title: Modéliser des distributions avec Python (Distribution Fitting)
collection: st
layout: single
author_profile: false
read_time: true
categories: [machinelearning]
excerpt : "Stats4Decision"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
    teaser: "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
search: false
toc_sticky: true
sidebar:
    nav: sidebar-sample
---

*Cet article suppose une connaissance préalable des lois de probabilités les plus communes*.

# Modéliser une distribution

> La modélisation d'une distribution (*probability distribution fitting*, ou *distribution fitting* en anglais) est le fait de trouver les paramètres de la loi de distribution de probabilité (ou de plusieurs lois candidates) qui correspond aux données que l'on cherche à modéliser. 

En d'autres termes, on souhaite savoir si nos données suivent par exemple une loi normale, une loi gamma, ou toute autre distribution, et les paramètres attachés à la loi. 

Pourquoi cherche-t'on a modéliser la distribution de nos données? L'information qu'apporte la distribution de nos données est en fait essentielle. Cela nous permet notamment de:
- déterminer la probabilité qu'un paiement à effectuer dans le futur dépasse un certain montant
- ou plus généralement, déterminer la fréquence d'occurence d'un certain phénomène

La modélisation de distributions est également souvent utilisée dans les logiciels d'exploration de données, car elle permet de comprendre les propriétés sous-jacentes de nos données.

La modélisation de distribution est une tâche qui peut s'effectuer de deux manières:
- Manuellement. Vous avez une idée de la loi de distribution, ou de quelques lois potentielles. La forme de la distribution vous permet d'en identifier quelques unes. Par la suite, vous trouvez les paramètres optimaux par maximum de vraisemblance (ou Maximum Likelihood Estimation, MLE en anglais), ou par la méthode des moments (Method of Moments) par exemple.
- Automatiquement. Vous utilisez une librairie ou un logiciel spécialisé, qui a déjà implémenté les maximums de vraisemblance de nombreuses lois, et cherchez à trouver la meilleure loi et les meilleurs paramètres d'un coup.

# Exemple avec Python

Supposons que vous travaillez dans une assurance, et disposez d'une série de données qui correspond aux montants des paiements que l'assurance a effectué pour les différents assurés de son portefeuille "Assurance Voiture". En clair, dès qu'un client qui est assuré chez vous a un accident avec son véhicule, le montant que vous lui remboursez est rentré dans la série de données. Il y aura typiquement beaucoup de paiements d'un petit montant, et quelques paiements d'un montant élevé. Cependant, on peut aussi s'attendre à ce que les plus petits montants ne soient pas déclarés par les assurés, car cela peut impacter leur bonus pour les années suivantes. Si l'on représente l'histogramme des données, voici ce à quoi on peut s'attendre:

![image](https://maelfabien.github.io/assets/images/s4d_1.png)

Nos hypothèses se confirment:
- pour des montants très faibles (<500€), les demandes de paiement sont faibles
- un pic est atteint pour des montants avoisinnant les 1'000€
- plus le montant augmente, moins il y a des cas de sinistres enregistrés

Avec Python, il est simple de réaliser une modélisation automatique de la distribution de nos données. C'est ce que nous allons voir maintenant! Tout d'abord, importez les librairies suivantes:

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import scipy
import scipy.stats
import time
```

Nous allons ensuite simuler de fausses données d'un portefeuille d'assurance, comme détaillé plus haut. Nous allons générer des données suivant une loi Gamma.

```python
# Nombre de valeurs à générer
length = 30000
bins=500

# Génération des données
data = np.random.gamma(2,1, length)

# Histogramme des données
y, x = np.histogram(data, bins=bins, density=True)
# Milieu de chaque classe
x = (x + np.roll(x, -1))[:-1] / 2.0
```

On peut alors représenter visuellement les données:

```python
plt.figure(figsize=(12,8))
plt.hist(data, bins=500, density=True)
plt.title("Montant des paiements effectués (K€)")
plt.show()
```

![image](https://maelfabien.github.io/assets/images/s4d_1.png)

Afin d'identifier, pour une loi de distribution donnée, les paramètres correspondant au maximum de vraisemblance, nous allons utiliser la librairie [`scipy`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html), qui offre près de 80 distributions dans sa version actuelle, et identifie les paramètres optimaux par maximum de vraisemblance.

Il suffit de spécifier la loi que l'on souhaite tester, et d'utiliser la méthode `fit` de Scipy pour récupérer les paramètres optimaux. Par exemple:

```python
dist_name = "gamma"

# Get parameters of the model
dist = getattr(scipy.stats, dist_name)

# Fit the model
param = dist.fit(data)
```

La variable `param` contient alors:

```
(1.993299446023943, -0.0005280086059209502, 0.9929125608037362)
```

Les paramètres sont donc les suivants:
- `shape` : 1.9933, contre une vraie valeur de 2
- `loc` : -0.0005, contre une vraie valeur de 0
- `scale` : 0.9929, contre une vraie valeur de 1


On peut alors re-tracer la distribution de probabilité de la loi Gamma avec ces paramètres.

```python
loc = param[-2]
scale = param[-1]
arg = param[:-2]

pdf = dist.pdf(x, loc=loc, scale=scale, *arg)

plt.figure(figsize=(12,8))
plt.plot(x, pdf, label=dist_name, linewidth=3) 
plt.plot(x, y, alpha=0.6)
plt.legend()
plt.show()
```

![image](https://maelfabien.github.io/assets/images/s4d_2.png)

La PDF (probability distribution function), ou densité, de la loi Gamma avec les paramètres identifiés par Scipy est très proche des données originales. Comment peut-on cependant mesurer la distance entre les deux courbes? Il existe plusieurs mesures de distance, mais une des plus populaires est la somme de résidus au carrés (sum of squared errors, en anglais). On peut calculer la somme des résidus au carré de cette manière:

```python
sse = np.sum((y - pdf)**2)
```

Ici, la somme des résidus au carré vaut `0.04362`. Ce chiffre nous permettra de sélectionner la meilleure loi
