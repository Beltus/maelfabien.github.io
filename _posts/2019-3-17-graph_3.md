---
published: true
title: Clustering Coefficient, Centrality and Community
collection: st
layout: single
author_profile: false
read_time: true
categories: [machinelearning]
excerpt : "Graph Analysis and Graph Learning"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
    teaser: "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
toc_sticky: true
---

So far, we covered the main kind of graphs, and the most basic characteristics to describe a graph. We'll now cover into more details graph analysis and the different ways a graph can be analyzed. 

<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

{% highlight python %}
{% endhighlight %}

For what comes next, open a Jupyter Notebook and import the following packages :

```python
import numpy as np
import random
import networkx as nx
from IPython.display import Image
import matplotlib.pyplot as plt
```

If you have not already installed the `networkx` paxkage, simply run :

```bash
pip install networkx
```

The following articles will be using the latest version  `2.x` of  `networkx`. NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.

# I. Clustering Coefficient

The clustering coefficient measures how well two nodes tend to cluster together. 

A **local** clustering coefficient measures how close a node $$ i $$  and its neighbors are to being a complete graph.

$$ C_i = \frac { Triangles_i} {Triples_i} $$

The local clustering coefficient is a ratio of the number of triangles centered at node $$ i $$ over the number of triples centered at node $$ i $$. 

![image](https://maelfabien.github.io/assets/images/clustering_coeff.png)

A **global** coefficient measures the density of triangles (local clusters) in the graph :

$$ CC = \frac {1} {n} \sum_i C_i $$

In the graph shown above, the clustering coefficient is equal to :

$$ CC = \frac {1} {5} ( 1 + 1 + \frac {1} {6} + 0 + 0 ) = \frac {13} {30} $$

## Erdos-Rényi

For Erdos-Rényi random graphs, $$ E[CC] = E[C_i] = p $$ where $$ p $$ the probability defined in the previous article. 

## Barabàsi - Albert

For Baràbasi-Albert random graphs, the global clustering coefficent follows a power law depending on the number of nodes. The average clustering coefficient of nodes with degree $$ k $$ is $$ C(k) \propto k^{-1} $$.

Nodes with low degree are connected to other nodes in their communitiy. Nodes with high degrees are linked to nodes in different communities. 

For a given graph, in `networkx`, the clustering coefficient can be easily computed. First, let's begin with the local clustering coefficients :

```python
# List of local clustering coefficients
list(nx.clustering(G_barabasi).values())
```

```
0.13636363636363635,
0.2,
0.07602339181286549,
0.04843304843304843,
0.09,
0.055384615384615386,
0.07017543859649122,
...
```
And average the results to find the global clustering coefficient of the graph :

```python
# Global clustering coefficient
np.mean(list(nx.clustering(G_barabasi).values()))
````
```
0.0965577637155059
```

# II. Community detection

Community detection is a process by which we partition the nodes into a set of groups / clusters according to a certain quality criterion. It is typically used to identify social communities, customers behaviors or web pages topics. 

A *community* is a set of nodes densely connected internally and/or sparsely connected externally. There is however no universal definition that one can give to define communities. 

![image](https://maelfabien.github.io/assets/images/community.png)

The process to identify communities is the following :
- define a quality criterion
- design an algorithm to optimize this criterion

We'll try to define this into more details.

## Notations

- Number of nodes in the graph : $$ n $$ 
- Nodes in the community : $$ S $$
- Number of edges in $$ S $$ : $$ m_s $$
- Number of edges in the graph : $$ m $$
- Number of nodes in $$ S $$ : $$ n_s $$
- Number of edges between $$ S $$ and the rest of the graph : $$ O_s $$

We are now ready to define quality criteria.

## Quality criteria

Quality criteria might be based on : 
- internal connections 
- external connections
- both

*Based on internal connections* :
- Internal density of edges : $$ \frac {m_s} {n_s \frac {(n_s-1)} {2}} $$
- Average internal degree : $$ \frac {2m_s} {n_s} $$

*Based on external connections* :
- Expansion : $$ \frac {O_s} {n_s} $$
- Ratio Cut : $$ \frac {O_s} {n_s ( n- n_s)} $$

*Based on both* :
- Conductance : $$ \frac {O_s} {2 m_s + O_s} $$
- Normalized cut : $$ \frac {O_s} {2m_s + O_s} + \frac {O_s} {2(m-m_s) + O_s} $$
- Modularity : $$ \frac {1} {4} (m_s - E(m_s)) $$

A note on modularity :
$$ E(m_s) $$ is computed with respect to a random process which preserves the degree of each node. Each degree is split into two parts. Each part is combined to another randomly. 

Modularity is defined by : $$ \frac {1} {2m} \sum_{i,j} (A_{ij} - \frac {d_i d_j} {2m} ) I_{C_i = C_j} $$

However, finding communities maximizing the modularity is an exponentially complex task in the number of groups. It can easily get very costly with a few hundred nodes. For this reason, methods such as the Louvain method have been developped.

We'll now cover computationally efficient techniques that scale pretty easily :

## Louvain Method

The pseudo-code of the Louvain method can be written this way :
- Assign a communitiy to each node at first
- Alternate the next 2 steps until convergence :
    - Optimize local modularity. For each node, create a new community with neighboring node maximizing modularity
    - Create a new weighted graph. Communities of the previous step become nodes of the graph
    
![image](https://maelfabien.github.io/assets/images/louvain.png)

Note that there is no theoretical guarantee of Louvain method, but it works well in practice.

## Hierarchical Clustering

In hierarchical clustering, we build a hierarchy of clusters. We represent the clusters under a form a dendrogram :

![image](https://maelfabien.github.io/assets/images/dendro.png)

The idea is to analyze community structures at different scales. We build the dendrogram bottom-up. We start with a cluster at each node, and merge the two "closest" nodes. 

But how do we measure close clusters ? We use similarity distances. Let $$ d(i,j) $$ be the length of the shortest path between $$ i $$ and $$ j $$. 
- Maximum linkage : $$ D( C_1, C_2 ) = min_{i ∈ C_1, j ∈ C_2} d(i,j) $$
- Average linkage : $$ D( C_1, C_2 ) = \frac {1} { \mid C_1 \mid \mid C_2 \mid } \sum_{i ∈ C_1, j ∈ C_2} d(i,j) $$
- Centroïd linkage : $$ D(C_1, C_2) = d(G_1, G_2) $$ where $$ G_1 $$ and $$ G_2 $$ are the centers of $$ C_1, C_2 $$.

The similarity distances can be illustrated as follows :

![image](https://maelfabien.github.io/assets/images/linkage.png)


> **Conclusion** : I hope that this article was helpful. Don't hesitate to drop a comment if you have any question.
