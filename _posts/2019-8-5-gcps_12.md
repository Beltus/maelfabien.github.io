---
published: false
title: Introduction to Cloud Dataproc - Week 1 Module 1
collection: bgd
layout: single
author_profile: false
read_time: true
categories: [bigdata]
excerpt : "Road to Google Cloud Platform Certification"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
    teaser: "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
toc_sticky: true
sidebar:
    nav: sidebar-sample
---

There are many sources of data :
- data that you analyze
- **data you collect but don't analyze**
- data you could collect but don't 
- data from 3rd parties

The main reason behind collecting data but not analyzing it is the fact that those data are often unstructured. It could include :
- free form text
- images
- calls from call center
- ...

We call unstructured data all the data that are not suited for the specific type of job we want to run on it. GIS data might be structured for Google Maps, but it's unstructured data for a relational database for example. Unstructured data might have a schema, a partial schema or no schema at all.

About 90% of enterprise data is unstructured. This is the way ML models for unstructured data are deployed on GCP :

![image](https://maelfabien.github.io/assets/images/gcp_152.png)

Those models, that are really data-intensive, and might include Cloud Vision or Speech-To-Text are trained by Google, and trained models are then uploaded to Cloud ML Engine using REST API.

# Why Cloud DataProc ?

When working with BigData, an efficient Hadoop-based architecture can be built on Cloud DataProc. Cloud DataProc is useful for Tera Bytes or Peta Bytes data levels. But how much is 1 PetaByte of data ?

![image](https://maelfabien.github.io/assets/images/gcp_153.png)

Some businesses will never need such capacities, and running Spark jobs would be more than enough. On the other hand, here's also what on PetaByte is :

![image](https://maelfabien.github.io/assets/images/gcp_154.png)

How do you scale to such large amount of data ?
- Vertical Scaling : more efficient single machines
- Horizontal Scaling : more machines running together

MapReduce arised when Google tried to index every single webpage on the Internet back in 2004.  It relies on 3 main steps :
- Map
- Shuffle 
- Reduce

Hadoop is the Apache solution of MapReduce. However, Hadoop has a major limitation, since the way design the job needs to be tuned for every job we must run. Apache Spark is a powerful and flexible way to process large datasets. Spark uses declarative programming : you tell the program what you want, and it figures out a way to make it happen. Spark can be used in Python, Java... It also comes with a ML library, called SparkML, and Spark SQL offers a SQL implementation on top of Spark.

![image](https://maelfabien.github.io/assets/images/gcp_155.png)

A typical Spark and Hadoop deployment involves :
- setting up the hardware and the Operating System software (OSS)
- optimizing the OSS
- debug the OSS
- process data

![image](https://maelfabien.github.io/assets/images/gcp_156.png)

When deploying a Hadoop cluster on-prem, lots of time is spent on administration and operational issues (monitoring, scaling, reliability...). One also needs to scale the cluster according to the utilization we make of it. 

DataProc is a manages service to run Hadoop on GCP. The different options to run Hadoop Clusters are the following :

![image](https://maelfabien.github.io/assets/images/gcp_157.png)

A typical Hadoop Dataproc deployment requires just 90 seconds before the cluster is up and running ! Cloud DataProc also supports Hadoop, Pig, Hive and Spark, and has high-level APIs for job submissions. It also offers connectors to BigTable, BigQuery and Cloud Storage.

# Create a Cloud DataProc Cluster


