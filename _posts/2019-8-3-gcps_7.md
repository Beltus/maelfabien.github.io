---
published: false
title: Predict Visitor Purchases with BigQuery ML - Week 1 Module 3
collection: bgd
layout: single
author_profile: false
read_time: true
categories: [bigdata]
excerpt : "Road to Google Cloud Platform Certification"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
    teaser: "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
toc_sticky: true
sidebar:
    nav: sidebar-sample
---

In Google Cloud, one can build Machine Learning models straight into BigQuery, using SQL ! Through this module, we will create Demand forecasting models using BigQuery ML.

# Introduction to BigQuery

BigQuery is an easy to use Data Warehouse. It's a petabyte scale fully-managed service, and has several advantages :
- it's serverless
- it offers flexible pricing models
- data encryption and security for regulatory requirements (restrictions on columns and row visibility
- geospatial data types and functions 
- foundation for BI and AI

It's a common workload to prototype ML models rapidly, and it is the bridge for data analysts and all users to your data.

![image](https://maelfabien.github.io/assets/images/gcp_79.png)

Big Query is essentially both a fast SQL query engine (BigQuery Query Service), and a managed storage for datasets (BigQuery Storage Service). Both services are fully managed, and linked by a Petabit network.

The storage service relies on Google Colossus file storage system. This storage system also powers Google Photos for example. This storage service can do both :
- bulk data ingestions (huge amount of data)
- and streaming data ingestion (real-time data)

To launch a Query service, you can do it :
- through the command line
- through the WebUI
- through 7 REST APIs

The Query Service runs interactive or batch queries. It can connect to CSV files in Cloud Storage, but also to other services through connectors (Cloud Dataproc or Google Sheets for example).

Both services run together to optimize the syntax of the SQL query that is ran. 

BigQuery offers a free monthly 1TB data processing.

# BigQuery Query Service

From the Query Editor, we'll explore San Francisco bikesharing data :

![image](https://maelfabien.github.io/assets/images/gcp_83.png)

Type the following command (with the back-hyphens) :

 ```
 `bigquery-public-data.san_francisco_bikeshare.bikeshare_trips`
```

This is the link to the data. On macOS, simply hold the Command button, and click on the link of the data :

![image](https://maelfabien.github.io/assets/images/gcp_84.png)

It opens a description of the table, gives you details about the number of rows, the size of the file... You can click on the Preview tab, without even running a query, to see the first 100 rows of the dataset.

![image](https://maelfabien.github.io/assets/images/gcp_85.png)

If you click on the "Query table" button, it creates the default SQL format query :

![image](https://maelfabien.github.io/assets/images/gcp_86.png)

If you don't want to type the name of the columns in the SELECT query, you can click on the column names from the Schema.

An example query would be :

```
SELECT trip_id, start_station_name FROM `bigquery-public-data.san_francisco_bikeshare.bikeshare_trips` LIMIT 1000
```

If you click on the Format button, it automatically re-formats the SQL query in a nice way :

![image](https://maelfabien.github.io/assets/images/gcp_87.png)

To get the stations in which we have the most rentals, we can run the following command :

```
# Top 10 station by Volume
SELECT
    start_station_name,
COUNT(trip_id) AS num_trips
FROM
    `bigquery-public-data.san_francisco_bikeshare.bikeshare_trips`
GROUP BY
    start_station_name
ORDER BY
    num_trips DESC
LIMIT
    1000
```

To filter only on rentals that occured after 2017, apply a filter on the WHERE :

```
# Top 10 station by Volume since 2018
SELECT
    start_station_name,
COUNT(trip_id) AS num_trips
FROM
    `bigquery-public-data.san_francisco_bikeshare.bikeshare_trips`
WHERE
    start_date > '2017-12-31 00:00:00 UTC'
GROUP BY
    start_station_name
ORDER BY
    num_trips DESC
LIMIT
    1000
```

![image](https://maelfabien.github.io/assets/images/gcp_88.png)

We can save a query as a Table to get a static table in result (not a view). To do so, create an empty database in your project's resources. I called mine `bikes`. Your architecture should be as follows :

![image](https://maelfabien.github.io/assets/images/gcp_89.png)

To save a table, simly add a CREATE OR REPLACE argument at first :

```
# Top 10 station by Volume since 2018
CREATE OR REPLACE TABLE bikes.after_2017
SELECT
    start_station_name,
COUNT(trip_id) AS num_trips
FROM
    `bigquery-public-data.san_francisco_bikeshare.bikeshare_trips`
WHERE
    start_date > '2017-12-31 00:00:00 UTC'
GROUP BY
    start_station_name
ORDER BY
    num_trips DESC
LIMIT
    1000
```

The table is then added to the `bikes` database ! To create a view, simply switch replace TABLE by VIEW :

```
# Top 10 station by Volume since 2018
CREATE OR REPLACE VIEW bikes.after_2017
SELECT
    start_station_name,
COUNT(trip_id) AS num_trips
FROM
    `bigquery-public-data.san_francisco_bikeshare.bikeshare_trips`
WHERE
    start_date > '2017-12-31 00:00:00 UTC'
GROUP BY
    start_station_name
ORDER BY
    num_trips DESC
LIMIT
    1000
```

To further explore the data, you can use DataStudio since it has a BigQuery connector.

Let's talk a little bit about IAM Project roles and security. There are several levels of content access managed through roles :
- Viewer : Can start a job in the project
- Editor : Can create a dataset in the project
- Owner : Can list all datasets in the project, delete andd create datasets

Dataset users should have the minimum permissions needed for their role. We use separated projects or datasets for different environments (DEV, QA, PRD...)

We also audit roles periodically.

# BigQuery Storage Service

In addition to super-fast a super-fast query system, BigQuery can also ingest data from a large variety of sources :
- Cloud Storage
- Google Drive
- Cloud Bigtable
- CSV, JSON...

BigQuery is automatically replicated, backed-up, set up to auto-scaling... It's a fully managed service. You can also directly query files that are outside the scope of BigQuery Managed Storage, although it's not as optimized as for the files inside the scope. This is typically useful when ingesting external datasets from other services of a company.

BigQuery also allows for streaming records through API. The max input file size is 1MB, and the max output is 1000 files per second per project (If need more, consider Cloud Bigtable). It allows to query data without waiting for a full batch load.

Big Query natively supports arrays as data types :

![image](https://maelfabien.github.io/assets/images/gcp_90.png)

It also supports STRUCTs, that satisfy the notions of Normalization of our tables.
