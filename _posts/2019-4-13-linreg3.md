---
published: true
title: Binary output prediction and Logistic Regression
collection: st
layout: single
author_profile: false
read_time: true
categories: [statistics]
excerpt : "Logistic Regression"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
    teaser: "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
toc_sticky: true
sidebar:
    nav: sidebar-sample
---

So far, with linear model, we have seen how to predict continuous variables. What happens when you want to classify with a linear model ?

{% highlight python %}
{% endhighlight %}

<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

# Linear Probability Model

Suppose that our aim is to do binary classification : $$ y_i = {0,1} $$. Let's consider the model :

$$ y = \beta_0 + \beta_1 X_1 + ... + \beta_k X_k + u $$

Where $$ E(u \mid X_1, ... X_k) = 0 $$. How can we peform binary classification with this model ? Let's start with a dataset in which you have binary observations and you decide to fit a linear regression on top of it. 

![image](https://maelfabien.github.io/assets/images/log_1.jpg)

Statistically speaking, the model above is incorrect :
- we would need to define a threshold under which we classify as 0, and above which we classify 1
- what if the values are greater than 1 ? Or smaller than 0 ?
- ...

Linear probability model has however one main advantage : the coefficients remain easily interpretable !

$$ \Delta P(Y=1 \mid X) = \beta_j \Delta X_j $$ 

Overall, this model needs to be adjusted / transformed to throw the predicted between values between 0 and 1. This is the main idea of the logistic regression !

# Logistic Regression

We are now interested in $$ P(Y=1 \mid X) = P(Y=1 \mid X_1, X_2, .. X_k) = G(\beta_0 + \beta_1 X_1 + ... + \beta_k X_k) $$. As you might guess, the way we define $$ G $$ will define the way we make our mapping. 
- If $$ G $$ is linear, this is obviously the linear model
- If $$ G $$ is a sigmoid : $$ G(z) = \frac {1} {1 + e^{-z}} $$, then the model is a logistic regression
- If $$ G $$ is a normal transformation $$ G(z) = \Phi(z) $$, then the model is a probit regression

In our case, we'll focus on the logistic regression.

## Sigmoid and Logit transformations

The **sigmoid** transformation is used to map values between 0 and 1 :

$$ Sig(z) = \frac {1} {1 + e^{-z}} $$

To understand precisely what this does, let's implement it in Python :

```python
import numpy as np
import matplotlib.pyplot as plt
import math

def sigmoid(x):
    a = []
    for item in x:
        a.append(1/(1+math.exp(-item)))
    return a
```

Then, plot if for a range of values of $$ X $$ :

```python
x = np.arange(-3., 3., 0.2)
sig = sigmoid(x)

plt.figure(figsize=(12,8))
plt.plot(x,sig, label='sigmoid')
plt.plot(x,x, label='input')
plt.title("Sigmoid Function")
plt.legend()
plt.show()
```

![image](https://maelfabien.github.io/assets/images/log_2.jpg)

The inverse transform is called the **logit** transform. It takes values that are in the range 0 to 1 and maps them to a linear form.

```python
def logit(x):
    a = []
    for item in x:
        a.append(math.log(item/(1-item)))
    return a
```

```python
plt.figure(figsize=(12,8))
plt.plot(x,sig, label='sigmoid')
plt.plot(x,logit(sig), label='logit tranform')
plt.title("Sigmoid - Logit Function")
plt.legend()
plt.show()
```

![image](https://maelfabien.github.io/assets/images/log_3.jpg)

## The logistic regression model

### Partial effect

In the logistic regression model : 

$$ P(Y=1) = \frac {1} {1 + exp^{-(\beta_0 + \beta_1 X_1 + ... + \beta_p X_p)}} $$

How can we interpret the partial effect of $$ X_1 $$ on $$ Y $$ for example ? Well, the weights in the logistic regression cannot be interpreted as for linear regression. We need to use the logit transform :

$$ \log( \frac {P(y=1)} {1-P(y=1)} ) = \log ( \frac {P(y=1)} {P(y=0)} ) $$ 
$$ = odds = \beta_0 + \beta_1 X_1 + ... + \beta_k X_k $$

We define the this ratio as the "odds". Therefore, to estimate the impact of $$ X_j $$ increasing by 1 unit, we can compute it this way :

$$ \frac {odds_{X_{j+1}}} {odds} = \frac {exp^{\beta_0 + \beta_1 X_1 + ... + \beta_j (X_j + 1) + ... + \beta_k X_k}} {exp^{\beta_0 + \beta_1 X_1 + ... + \beta_j X_j + ... + \beta_k X_k}} $$

$$ = exp^{\beta_j (X_j + 1) - \beta_j X_j} = exp^{\beta_j} $$

A change in $$ X_j $$ by one unit increases the log odds ratio by the value of the corresponding weight.




The Github repository of this article can be found [here](https://github.com/maelfabien/Machine_Learning_Tutorials).


Sources :
- [Interpretable Machine Learning book](https://christophm.github.io/interpretable-ml-book/logistic.html)
