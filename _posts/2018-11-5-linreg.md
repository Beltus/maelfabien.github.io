---
published: true
title: A full guide to Linear Regression
collection: st
layout: single
author_profile: false
read_time: true
categories: [statistics]
excerpt : "Linear Model"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
    teaser: "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
toc_sticky: true
sidebar:
    nav: sidebar-sample
---

Before starting this series of articles on Machine Learning, I thought it might be a good idea to go through some Statistical recalls. This first article is an introduction to some more detailed articles on statistics. I will be illustrating some concepts using Python codes. 

{% highlight python %}
{% endhighlight %}

<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

## Framework

The most basic statistical fundation of machine learning models is the following : $$ Y_i =  f(X_i) + {\epsilon} $$ . This simple equation states the following :
- we suppose we have n observations of a dataset, and we pick on $$ i^{th} $$
- $$ Y_i $$ is the output of this observation, called the target
- $$ X_i $$ is called a feature and is an independent variable we observe
- $$ f $$ is the real model that states the link between the features and the output
- $$ {\epsilon} $$ is the noise of the model. The datas we observe usually do not stand on a straight line, because there are variations of the measure in real life. 

The framework we consider here is pretty simple. We only have one feature per observation. In real life, we do usually have several features per observation. An example of this might be the following : you work as a data scientist in an insurance company. You would typically have several informations on each customer : name, address, age, family, car, salary...

## Statistical motivation

Suppose that we have n observations, and for each observation i, we have one feature $$ X_{i} $$. 

Our role is to identify the link between $$ Yi $$ and $$ X_{i} $$ in order to be able to predict $$ Y_{n+1} $$ . To build our prediction, we need to identify :
- a model, which is the link between $$ Y_i $$ and the features, represented here by the function $$ f $$ in :
$$ Y_i =  f(X_i) + {\epsilon} $$ . In real life, this model $$ f $$ is unknown in real life and is estimated by : $$ \hat{Y}_i =  \hat{f}(X_i) $$ where the hat describes an estimation. We try to estimate $$ \hat{f} $$ that corresponds to real life $$ f $$.
- we also need to identify parameters, which will help our prediction to get as close as possible to real datas. The parameters are usually set in order to minimize the distance between our model and the datas, i.e they are set so that the derivative of a loss function (for example : $$ \sum(\hat{Y_i} - Y_i)^2 ) $$ is equal to 0.

The true model and the true parameters in real life are unknown. For this reason, we make a selection among several models (linear or non-linear). The model selection is usually based on an exploratory data analysis. An article will further develop this question.

# Linear Regression in 1 dimension

From now on, we will focus on the most basic model called the unidimensional linear regression. This model states that there is a linear link between the features and the dependent variable. 

The true model we expect is the following :  $$ Y_i = {\beta}_0 + {\beta}_1{X}_{i} + {\epsilon}_i $$
- $$ {\beta}_0 $$ is called the intercept, it is a constant term
- $$ {\beta}_1 $$ is the coefficient associated with $$ X_i $$ . It describes the weight of $$ X_i $$ on the final output.
- $$ {\epsilon}_i $$ is called the residual. It is a white noise term that explains the variability in real life datas. 

The hypothesis on $$ {\epsilon} $$ are :
- $$ E({\epsilon}) = 0 $$ , i.e a white noise condition
- $$ {\epsilon}_i ∼ iid  {\epsilon} $$ for all i = 1,...,n, i.e a homoskedasticity condition

However, the true parameters remain unknown as these are the ones we expect to estimate. Therefore, the model we estimate is the following : $$ \hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1{X}_{i} $$ . 

A classic example would me to try to predict the average housing market price in the economy based on the GDP for example. We would expect pretty much a linear relationship between both. You can find the file I will be using [here]( https://maelfabien.github.io/assets/files/housing.txt)

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import sklearn.linear_model as lm
import math
from scipy import stats

df = pd.read_csv('housing.txt', sep=" ")
df
```
![image](https://maelfabien.github.io/assets/images/Tab1.png){:height="20%" width="20%"}

```python
#Scatter plot
plt.figure(figsize=(7,5))
plt.scatter(df['gdp'], df['house'])

#On ajoute les titres des axes
plt.xlabel("Gross Domestic Product)", fontsize = 12)
plt.ylabel("Housing price", fontsize = 12)
plt.title("Relation between GDP and Average housing prices")

plt.show()
```
![image](https://maelfabien.github.io/assets/images/Graph1.png){:height="60%" width="60%"}

Our intuition was right. The relationship looks linear between the GDP and the average housing price. How do we move on from here ?

## Parameter estimate

Now that we know the relationship looks linear, the next step is to estimate the coefficients $$ \hat{\beta}_0 , \hat{\beta}_1 $$ in order to draw a line that fits our datas. In the linear regression, estimating the parameter means identifying the Betas : $$ \hat{\beta}_0 , \hat{\beta}_1 $$ so that they minimize the distance with the real datas : 

$$ argmin \sum(Y_i - \hat{\beta}_0 - \hat{\beta}_1{X}_{i})^2 $$ .

> This approach is know as the Ordinary Least Squares (OLS). We minimize the square of the distance between each data point and our estimate. Why the square ? Because the derivative is easier to find, and we take both positive and negative deviations into account.

The solution of the OLS problem in 1 dimension is the following :
$$ \hat{\beta}_1 = \frac{\sum(X_i – \bar{X}) (Y_i – \bar{Y})} {\sum(X_i – \bar{X})^2} $$
$$ \hat{\beta}_0 = \bar{Y} – \hat{\beta}_1 \bar{X} $$

We can compute those results in Python :

```python
# Slope Beta 1 :
x = df['gdp']
x_bar = mean(df['gdp'])
y = df['house']
y_bar = mean(df['house'])

beta1 = ((x - x_bar)*(y-y_bar)).sum() / ((x-x_bar)**2).sum()
print("Beta_1 coefficient estimate : " + str(round(beta1,4)))
```
`0.1012`

```python
beta0 = y_bar - beta1 * x_bar
print("Beta_0 coefficient estimate : " + str(round(beta0,4)))
```
`-1794.0861`

We can plot the estimated linear regression to assess whether it looks good or not :
```python 
#Scatter plot
plt.figure(figsize=(7,5))
plt.scatter(df['gdp'], df['house'])
plt.plot(df['gdp'], beta1 * df['gdp'] + beta0, c='r')
#On ajoute les titres des axes
plt.xlabel("Gross Domestic Product)", fontsize = 12)
plt.ylabel("Housing price", fontsize = 12)
plt.title("Relation between GDP and Average housing prices")

plt.show()
```
![image](https://maelfabien.github.io/assets/images/Graph2.png){:height="60%" width="60%"}

It looks great ! There is a pretty efficient way to do this using Scikit Learn built-in functions. 
```python
x_1 = df[['gdp']]
skl_linmod = lm.LinearRegression(fit_intercept = True).fit(x_1,y)

beta1_sk = skl_linmod.coef_[0]
beta0_sk = skl_linmod.intercept_
```

This will head exactly the same results. Alright, so far we are satisfied by how our estimate looks like. But how do we assess how good our estimate is?

## Assess accuracy of the estimate using R2

A common metric used to assess the overall fit of our model is the $$ R^2 $$ metric (pronounced R squared). The $$ R^2 $$ measures the percentage of the variance of the datas we manage to explain with our model :

$$ {R^2} = \frac{\sum(Y_i – \hat{Y})^2} {\sum(Y_i – \bar{Y})^2} $$

```python
r_carre = 1 - (((y-(beta0 + beta1 * x))**2).sum())/((y-y_bar)**2).sum()
```
`0.861`

The $$ R^2 $$ takes values between 0 and 1. The closer we are to 1, the more variance we tend to explain. However, adding variables will always make the $$ R^2 $$ increase, even if the variable we added is not related to the output at all. I do invite you to check the adjusted - $$ R^2 $$ if you would like to know more on how to handle this issue.

## Are coefficients significant ?

Alright, we have now fitted our regression line and estimated the goodness of the fit. But all the variables included in the model might not be worth including ?
What if for example $$ {\beta}_1 $$ is not significantly different from 0 ?

Both parameters $$ {\beta}_0, {\beta}_1 $$can be described by three metrics :
- an expectation
- a bias
- a variance

The expectation of the parameter corresponds... to its expected value. Nothing really new here : $$ E(\hat{\beta}_j) $$

The bias corresponds to how far we are from the actual value. It is given by : $$ E(\hat{\beta}_j) - {\beta}_j $$ . The true bias is typically unknown, as we try to estimate $$ {\beta}_j $$ . If the biais is 0, we say that the estimator is unbiaised.

The variance defines the stability of our estimator regarding the observations. Indeed, the features might be highly spread, which would mean a pretty big variance. 

It can be shown that the variance of $$ {\beta}_1 $$ is given by :
$$ \hat{\sigma}{_\hat{\beta_1}} = \frac{\hat{\sigma}} {\sqrt{\sum(X_i – \bar{X})^2}} $$

And the one of  $$ {\beta}_0 $$ by :
$$ \hat{\sigma}{_\hat{\beta_0}} = \hat{\sigma} \sqrt{\frac{1} {n} + \frac{\sum(X_i)^2} {\sum(X_i – \bar{X})^2}} $$

Where the estimated variance $$ \hat{\sigma} $$ is defined by : $$ \hat{\sigma} = \sqrt\frac{\sum(Y_i – \hat{Y}_i)^2} {n – p-1} $$ . This is an unbiaised estimator of the variance.

Now, we do have all the elements required to compute the standard errors of our parameters :
```python
sigma2 = math.sqrt(np.var(y))
sigma_beta1 = math.sqrt(sigma2 / ((x-x_bar)**2).sum())
```
`0.00264`

```python
sigma_beta0 = math.sqrt(sigma2 * (1/len(y) + (x_bar**2)/((x-x_bar)**2).sum()))
```
`54.09148485682019`

Graphically, biais and variance can be represented this way :
![image](https://maelfabien.github.io/assets/images/bias.png){:height="50%" width="50%"}

Why are we spending time on those metrics ?
Actually, those metrics allow us to compute what we call test hypothesis.

## Test Hypothesis

For each parameter, we want to test whether the parameter in question has a real impact on the output or not, to avoid adding dimensions that bring no significant information. In the linear regression : $$ \hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1{X}_{i} $$ , it would mean testing whether the Betas are significantly different from 0 or not. 

To do so, we proceed to a statistical test. If our aim is the state if the parameter is significantly different from 0, we are doing a test with :
$$ H_0 $$ the null hypothesis : $$ {\beta}_j = 0 $$ 
and $$ H_1 $$ the alternative hypothesis : $$ {\beta}_j ≠ 0 $$ . 

> Some further theory is needed here : Recall the Central Limit Theorem.
$$ \sqrt{n} \frac{\bar{Y}_n - {\mu}} {\sigma} $$ converges to $$ ∼ {N(0,1)} $$ as n tends to infinity if $$ {\sigma} $$ is knowm.

In case $$ {\sigma} $$ is unknown, Slutsky's Lemma states that $$ \sqrt{n} \frac{\bar{Y}_n - {\mu}} {\hat{\sigma}} $$ converges to $$ ∼ {N(0,1)} $$ if $$ {\hat{\sigma}} $$ converges to $$ {\sigma} $$ .

Most of the time, $$ {\sigma} $$ is unknown. From this point, it can be shown that :
$$ \hat{T}_j = \frac{\hat{\beta}_j - 0} {\hat{\sigma}_j} ∼ {\tau}_{n-p-1} $$ where $$ {\tau}_{n-p-1} $$ and $$ n-p-1 $$ is the degrees of freedom (p is the dimension, equal to 1 here).

This metric is called the T-Stat, and it allows us to perform an hypothesis test. The 0 in the numerator can be replace by any value we would like to test actually. 

How to interpret the T-Stat ?

The T-Stat should be compared with the Critical Value. The critical value is the quantile of the corresponding Student distribution at a given level $$ {\alpha} $$. If a coefficient is significant at a level $$ {\alpha} $$ , this means that the T-Stat is above or under the quantiles of the Student Distribution.

![image](https://maelfabien.github.io/assets/images/Graph3.png){:height="50%" width="50%"}

Another interpretation is that the probability that the coefficient estimate is not in the interval $$ [- {t}_{1-{\alpha}/2}; + {t}_{1-{\alpha}/2} ] $$ is smaller than $$ {\alpha} $$ . This probability is called the p-value and is defined by :
$$ p_{value} = Pr( |\hat{T}_j| > |{t}_{1-{\alpha}/2}|) $$

Thus, in a null test, we do reject $$ H_0 $$ at a level $$ {\alpha} $$ (typically 5%) if and only if the p-value is smaller than $$ {\alpha} $$ , i.e the probability to be in the interval around 0 is really small.

```python
from scipy import stats

t_test = beta1 / sigma_beta1
print("The T-Stat is : " + str(round(t_test,4)))

p = (1 - stats.t.cdf(abs(t_test), len(df)-2)) * 2
print("The p-value is : " + str(round(p,10)))
```
`The T_stat is : 38.3198`
`The p-value is : 0.0`

In our example, the T-Stat is pretty big, bigger than the corresponding quantile at 5% of the Student distribution, and the p-value is by far smaller than 5%. Therefore, we reject the null hypothesis : $$ H_0 : {\beta}_1 = 0 $$ and we conclude that the parameter $$ {\beta}_1 $$ is not null.

## Confidence Interval for the parameters

Finally, a notion one should get familiar with is the notion of confidence interval. Using the CLT, one can set a confidence interval around an estimate of a parameter.

The lower bound and the upper bound are determined by the critical value of the student distribution at a level $$ {\alpha} $$, and by the standard deviation of the parameter.
$$ {\beta}_1 ± {t}_{1-{\alpha}/2} * \hat{\sigma}_{\hat{\beta_1}} $$

```python
t_crit = stats.t.ppf(0.975,df=len(y) - 2)

c0 = beta1 - t_crit * sigma_beta1
c1 = beta1 + t_crit * sigma_beta1
```
`0.0954, 0.1068`

The exact same process can be done for the parameter $$ {\beta}_0 $$

## Type I and Type II risks

Rejecting or not an hypothesis comes at a cost : one could possibly missclassify a parameter. 

The Type I risk is the probability to reject $$ H_0 $$ whereas it is true.
The Type II risk is the probability to not reject $$ H_0 $$ whereas it is false.

The Level of a test is 1 - Type I risk, and represents the probability to not-reject $$ H_0 $$ when $$ H_0 $$ is true.

Wikipedia summarizes this concept pretty well :
![image](https://maelfabien.github.io/assets/images/Graph4.png){:height="50%" width="50%"}

## Confidence Interval for the model

We can define two types of confidence intervals for the model. The first one is the estimation error confidence interval. We need to take into account the error around $$ {\beta_1} $$ and around $$ {\beta_0} $$ : 

$$ CI(x) = \hat\beta_0 + \hat\beta_1 x \, \pm \, t_{1 - \frac{\alpha}{2}}^{(n-2)} \, \hat\sigma\sqrt{\frac{1}{n} + \frac{(x - \overline{x})^2}{\sum_{i=1}^n(x_i - \overline{x})^2}} $$

Moreover, the prediction risk is different. The prediction risk corresponds to the risk associated with the extension of our model onto a new prediction : $$ Y_{new} = \hat{Y} + {\epsilon} ; {\epsilon} ∼ N(0,{\sigma}^2) $$ . We have a new source of volatility : $$ {\epsilon} $$ . And this leads to a modified confidence interval :

$$ PI(x) = \hat\beta_0 + \hat\beta_1 x \, \pm \, t_{1 - \frac{\alpha}{2}}^{(n-2)} \, \hat\sigma\sqrt{1 + \frac{1}{n} + \frac{(x - \overline{x})^2}{\sum_{i=1}^n(x_i - \overline{x})^2}} $$

These two confidence intervals can be plotted on the previous graph :

```python
plt.figure()
y = df['house']
plt.scatter(df['gdp'], df['house'], label = "Data", color = "green")
plt.plot(df['gdp'], beta0 + beta1 * x, label = "Regression", color = "black")

plt.fill_between(df['gdp'], beta0 + beta1 * x - t_crit * np.sqrt(sigma2 * (1 + 1/len(y) + (x - x_bar)**2/((x-x_bar)**2).sum())), beta0 + beta1 * x + t_crit * np.sqrt(sigma2 * (1 + 1/len(y) + (x - x_bar)**2/((x-x_bar)**2).sum())), color = 'blue', alpha = 0.1, label = '90% PI')
plt.fill_between(df['gdp'], beta0 + beta1 * x - t_crit * np.sqrt(sigma2 * (1/len(y) + (x - x_bar)**2/((x-x_bar)**2).sum())), beta0 + beta1 * x + t_crit * np.sqrt(sigma2 * (1/len(y) + (x - x_bar)**2/((x-x_bar)**2).sum())), color = 'red', alpha = 0.4, label = '90% CI')

plt.legend()
plt.xlabel("GDP", fontsize = 12)
plt.ylabel("Housing price", fontsize = 12)
plt.title("Relation between GDP and Average housing prices")
plt.xlim(min(x), max(x))
plt.show()
```

![image](https://maelfabien.github.io/assets/images/Graph5.png){:height="60%" width="60%"}

# Linear Regression in 2 dimensions

So far, we have covered the unidimensional linear regression framework. But as you might expect, this is only a simple version of the linear regression model. Back to our housing price problem. So far, we only included the GPD variable. But as you may know, interest rates are also a major leverage on the housing market. 

## Framework

The more general framework can be described as : $$ Y =  X {\beta}+ {\epsilon} $$ . Notice that we are now in matrix form. Indeed :
- we suppose we have n observations of a dataset, and p features per observation
- $$ Y $$ is the output of this observation
- $$ {\beta} $$ is the column vector that contains the true parameters
- $$ X $$ is now a matrix of dimension (nxp)

Otherwise, the conditions on $$ {\epsilon} $$ remain the same. 
- $$ E({\epsilon}) = 0 $$ , i.e a white noise condition
- $$ {\epsilon}_i ∼ iid  {\epsilon} $$ for all i = 1,...,n, i.e a homoskedasticity condition

In the 2 dimension problem introduced above to predict the housing prices, our task is now to estimate the following model :  $$ \hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1{X}_{1i} + \hat{\beta}_2{X}_{2i} $$

The dataset I'm using can be found here  : https://maelfabien.github.io/myblog/files/housing.txt

```python
import pandas as pd
import numpy as np
import math

df = pd.read_csv('https://maelfabien.github.io/myblog/files/housing.txt', sep=" ")
df
```
![image](https://maelfabien.github.io/assets/images/Tab2.png){:height="35%" width="35%"}

```python
fig, axs = plt.subplots(2, figsize=(8,8))

axs[0].scatter(df['gdp'], df['house'])
axs[1].scatter(df['interests'], df['house'], c='r')

axs[0].set_xlabel("Gross Domestic Product", fontsize = 12)
axs[0].set_ylabel("Housing Price", fontsize = 12)
axs[0].set_title("Relation between GDP and Average Housing Prices")

axs[1].set_xlabel("Interest rates", fontsize = 12)
axs[1].set_ylabel("Housing Price", fontsize = 12)
axs[1].set_title("Relation between interest rates and Average Housing Prices")

plt.tight_layout()

plt.show()
```
![image](https://maelfabien.github.io/assets/images/Graph6.png){:height="70%" width="70%"}

The linear relationship with the interest rate seems to hold again ! Let's dive into the model. 

## OLS estimate

Recall that to estimate the parameters, we want to minimize the sum of squared residuals. In higer dimension, the minimization problem is :

$$ argmin \sum(Y_i - X \hat{\beta})^2 $$ .

In our specific example, $$ argmin \sum(Y_i - \hat{\beta}_0 - \hat{\beta}_1 {X}_{1i} - \hat{\beta}_2 {X}_{2i})^2 $$ 

> The general solution to OLS is : $$ \hat{\beta} = {(X^TX)^{-1}X^TY} $$ . Simply remember that this solution can only be derived if the Gram matrix defined by $$ X^TX $$ is invertible. Equivalently, $$ Ker(X) = {0} $$ . This condition guarantees the **uniqueness** of the OLS estimator. 

```python
x = np.hstack([np.ones(shape=(len(y), 1)), df[['gdp', 'interests']]]).astype(float)
xt = x.transpose()
X_gram = x
gram = np.matmul(xt, x)
i_gram = inv(gram)
```
We can use the Gram matrix to compute the estimator $$ \hat{\beta} $$ :
```python
Beta = np.matmul(inv(gram), np.matmul(xt,y))

#Display coefficients Beta0, Beta1 and Beta2
print("Estimator of Beta0 : " + str(round(Beta[0],4)))
print("Estimator of Beta1 : " + str(round(Beta[1],4)))
print("Estimator of Beta2 : " + str(round(Beta[2],4)))
```
`Estimator of Beta0 : -1114.2614`
`Estimator of Beta1 : -0.0079`
`Estimator of Beta2 : 21252.7946`

We will discuss cases in which this condition is not met in the next article. 

## Orthogonal projection

Note that $$ \hat{Y} = X \hat{\beta} = X(X^TX)^{-1}X^TY = H_XY $$ where $$ H_X = X(X^TX)^{-1}X^T $$ .
We say that $$ H_X $$ is an orthogonal projector since it meets two conditions :
- $$ H_X $$ is symmetrical : $$ {H_X}^T = H_X $$
- $$ H_X $$ is idempotent : $$ {H_X}^2 = H_X $$

Graphically, this projection states that we project $$ \hat{Y} $$ onto the hyperplane formed by the columns of X.
![image](https://maelfabien.github.io/assets/images/Graph7.png){:height="70%" width="70%"}

This allows us to redefine the vectors of residuals :
$$ {\epsilon} = Y - \hat{Y} = (I - H_X)Y $$ where $$ I $$ is the identity matrix.

$$ (I - H_X) $$ is an orthogonal projector on the orthogonal of $$ Vect(X) $$ .

## Bias and Variance of the parameters

Notice that $$ \hat{\beta} - {\beta} = (X^TX)^{-1}X^TY - (X^TX)^{-1}X^TX{\beta} = (X^TX)^{-1}X^T{\epsilon} $$

The bias of the parameter is defined by : $$ E(\hat{\beta}) - {\beta} = E(\hat{\beta} - {\beta}) = E((X^TX)^{-1}X^T{\epsilon}) = (X^TX)^{-1}X^TE({\epsilon}) = 0 $$ .
The estimate $$ \hat{\beta} $$ of the parameter $$ {\beta} $$ is **unbiaised**.

The variance of the parameter is given by : $$ Cov(\hat{\beta}) = (X^TX)^{-1}X^TCov({\epsilon})X(X^TX)^{-1} = (X^TX)^{-1}{\sigma}^2 $$ .
Therefore, for each $$ {\beta}_j $$ : $$ \hat{\sigma}^2_j = \hat{\sigma}^2{(X^tX)^{-1}_{j,j}} $$

```python
#Standard errors of the coefficients
sigma = math.sqrt(((y - Beta[0] - Beta[1] * xt[1] - Beta[2]*xt[2])**2).sum()/(len(y)-3))
sigma2 = sigma ** 2

se_beta0 = math.sqrt(sigma2 * inv(gram)[0,0])
se_beta1 = math.sqrt(sigma2 * inv(gram)[1,1])
se_beta2 = math.sqrt(sigma2 * inv(gram)[2,2])

print("Estimator of the standard error of Beta0 : " + str(round(se_beta0,3)))
print("Estimator of the standard error of Beta1 : " + str(round(se_beta1,3)))
print("Estimator of the standard error of Beta2 : " + str(round(se_beta2,3)))
```
`Estimator of the standard error of Beta0 : 261.185`
`Estimator of the standard error of Beta1 : 0.033`
`Estimator of the standard error of Beta2 : 6192.519`

The significance of each variable is assessed using an hypothesis test : $$ \hat{T}_j = \frac{\hat{\beta}_j} {\hat{\sigma}_j} $$
```python
#Student t-stats
t0 = Beta[0]/se_beta0
t1 = Beta[1]/se_beta1
t2 = Beta[2]/se_beta2
print("T-Stat Beta0 : " + str(round(t0,3)))
print("T-Stat Beta1 : " + str(round(t1,3)))
print("T-Stat Beta2 : " + str(round(t2,3)))
```
`T-Stat Beta0 : -4.266`
`T-Stat Beta1 : -0.242`
`T-Stat Beta2 : 3.432`

Something pretty interesing happens here : by adding a new variable to the model, the coefficient of $$ {\beta}_1 $$ quite logically changed, but became non-significant. We do expect $$ {\beta}_2 $$ to explain the model way better now.


The Quadratic Risk is : $$ E[(\hat{\beta} - {\beta})^2] = {\sigma}^2 \sum{\lambda}_k^{-1} $$ where $$ {\lambda}_k $$ are the eigenvalues of the Gram matrix.

The prediction risk is : $$ E[(\hat{Y} - Y)^2] = {\sigma}^2 (p+1) $$

# Random Design Matrix

So far, a hidden hypothesis was actually set without being explictly defined : $$ X $$ should be deterministic. Indeed, we should be able to have a full control on how $$ X $$ is measured. This is what we call a fixed design matrix. Let me quickly introduce an extension of what has been covered so far.

## Random Design

In real life situations, $$ X $$ is rarely fixed. Indeed, many external components might influence the measures you are currently making. For example, some goelogical datas collected over time could depend on the temperature, the atmospheric pressure or other external factors... And it might not be taken into account.

For this reason, we introduce the random design matrices where : $$ X_i = X_i(w) $$ is a random variable. The hypothesis on $$ \epsilon $$ slightly change :
- $$ E({\epsilon}_i|X_i) = 0 $$ 
- $$ Var({\epsilon}_i|X_i) = {\sigma}^2 $$


## Consequences

The minimization problem remains the same as previously :
$$ argmin \sum(Y_i - X \hat{\beta})^2 $$ .

The difference relies in the associated theoretical results. Recall the Gram matrix that we previously defined :
$$ G = X^TX $$. 
It should now be written as follows :
$$ G = E(X^TX) $$.

The limit results become :
$$ \frac {1} {\sqrt{n}} * ({\hat{\sigma}_n - {\sigma}}) $$ tends as n goes to infinity to $$ N(0, E(X^TX)^{-1}{\sigma}^2) $$

Notice that it is now a limit result. 

## What changes ?

What should be remembered from this small article ? Well the direct consequence we face in this case is a change in the condifence interval as the limit distribution is not the same as previously.

Previously, we had : 

$$ {\beta}_{n,k} ± {t}_{1-{\alpha}/2} * \hat{\sigma}_n $$, 
$$ t $$ being the quantile of the student distribution

This result is constrained to the gaussian hypothesis, which is quite restrictive. 

Now that we introduced the random design matrix, the confidence interval becomes :

$$ {\beta}_{n,k} ± ({\Phi}^{-1})_{1-{\alpha}/2} * \hat{\sigma}_n $$ , $$ {\Phi}^{-1} $$ being the quantile of the Normal distribution. 

# Normal Regression Model

## Concept

Recall that in a multi-dimensional linear regression, we have :  $$ Y =  X {\beta}+ {\epsilon} $$

And the following conditions on $$ {\epsilon} $$ :
- $$ E({\epsilon}) = 0 $$ , i.e a white noise condition
- $$ {\epsilon}_i ∼ iid  {\epsilon} $$ for all i = 1,...,n, i.e a homoskedasticity condition

Notice also that we never specified any distribution for $$ {\epsilon} $$. This is where the Normal law comes in. This time, we make the hypothesis that : $$ {\epsilon}_i ∼ iid  N(0, {\sigma}^2) $$. 

Independence implies that :
- $$ E({\epsilon}_i|X_i) = E({\epsilon}) = 0 $$
- $$ Var({\epsilon}_i|X_i) = {\sigma}^2 $$

A joint normality is not required in the normal regression model. We simply want to have that the conditional distribution of $$ Y $$ given $$ X $$ is normal.

## Maximum Likelihood Estimation (MLE)

First, we should observe that the previous condition on the conditional distribution of $$ Y $$ can be translated into :

$$ f(y|X) = \frac {1} {(2{\pi}{\sigma}^2)^{1/2}} e^{- \frac {1} {2{\sigma}^2} (Y - X {\beta})^2} $$

Under the assumption that the observations are independent, the conditional density becomes :
$$ f(y_1, y_2, ... | x_1, x_2,...) = \prod {f(y_i | x_i)} $$

$$ = \prod {\frac {1} {(2{\pi}{\sigma}^2)^{1/2}} e^{- \frac {1} {2{\sigma}^2} (y_i - x_i {\beta})^2} } $$

$$ = \frac {1} {(2{\pi}{\sigma}^2)^{n/2}} e^{- \frac {1} {2{\sigma}^2} \sum (y_i - x_i {\beta})^2} $$

$$ = L({\beta}, {\sigma}^2) $$

$$ L $$ is called the likelihood function. Our aim is to find the Maximum Likelihood Estimation (MLE), i.e the values of $$ {\beta} $$ such that the likelihood function is maximal. A natural interpretation is to identify the values of $$ {\beta}, {\sigma}^2 $$ that are the most likely. We can sum up the maximization problem as follows :

$$ ( \hat{\beta}, \hat{\sigma}^2 ) = {argmax}_{ ({\beta}, {\sigma}^2) }  L( {\beta}, {\sigma}^2 ) $$

You might have noticed that it is not always simple to work with products in a likelihood function. Therefore, we introduce the log-likelihood function : 

$$ l({\beta}, {\sigma}^2) = log L({\beta}, {\sigma}^2) $$

$$ = log f(y_1, y_2, ... | x_1, x_2,...) $$

$$ = - \frac {n} {2} log (2{\pi}{\sigma}^2) - \frac {1} {2{\sigma}^2} \sum (y_i - x_i {\beta})^2 $$

The maximization problem can be re-expressed as :

$$ (\hat{\beta}, \hat{\sigma}^2) = {argmax}_{( {\beta}, {\sigma}^2 )}  log L({\beta}, {\sigma}^2) $$

## First Order conditions

The MLE is usually identified numerically. In our case, we can explore it algebraically, by identifying $$ \hat{\beta}, \hat{\sigma}^2 $$ that jointly solve the First Order Conditions :

$$ (1) : \frac {d} {d {\beta}} log L({\beta}, {\sigma}^2) = 0 $$

$$ (2) : \frac {d} {d {\sigma}} log L({\beta}, {\sigma}^2) = 0 $$

It can be pretty easily shown that the MLE will give us the same results as the OLS procedure. Indeed, $$ \hat{\beta} = {(X^TX)^{-1}X^TY} = \hat{\beta_{OLS}} $$, and $$ $$ \hat{\sigma}^2 = \frac {1} {n} \sum (y_i - x_i {\beta})^2 = \hat{\sigma}_{OLS}}^2 $$

The last step is to plug-in the estimators in the initial problem :

$$ l({\beta}, {\sigma}^2) = - \frac {n} {2} log (2{\pi}{\sigma}^2) - \frac {1} {2{\sigma}^2} \sum (y_i - x_i {\beta})^2 $$

$$ = - \frac {n} {2} log  (2{\pi}\hat{\sigma}^2) - \frac {n} {2} $$

# Pseudo Least Squares

Do you remember when we defined the Gram matrix as $$ X^TX $$ ? To define the OLS estimator, we defined the Gram matrix as invertible. In other words, $$ Ker(X) = {0} $$ . But what happens when it is not the case ?

## When does it happen ?

It might happen that the Gram matrix is not invertible. But what would that mean? And how does it happen?
- it arises from a non-unique OLS solution
- it typically is the case when the dimension of our design matrix $$ X $$ is larger than the number of observations itself, e.g collecting a lot of datas per patient in a small medical study
- it implies that the pseudo-inverse should be used instead of the inverse of the Gram matrix


## Singular Value Decomposition

If we cannot invert the Gram matrix, we are stuck in the algeraic derivation of the OLS model at the following expression :

$$ (X^TX){\beta} = X^TY $$

We do not have a unique solution, but a whole set of solutions defined by : $$ \hat{\beta} + Ker(X) $$.

In order to compute the inverse of $$ X^TX $$, we need to use the Singular Value Decomposition (SVD). The SVD is a matrix decomposition theorem that states the following :

> Any matrix $$ X_{(n*n)} $$ can be de decomposed as $$ X = USV^T $$ 
Where : 
- $$ U $$ is a $$ (n*n) $$ matrix, and $$ U^TU = I_n $$, contains left singular vectors
- $$ S $$ is a $$ (n*p) $$ matrix that contains the eigen values of X on the diagonal, and 0s elsewhere.
- $$ V $$ is a $$ (p*p) $$ matrix, and $$ V^TV = I_p $$, contains right singular vectors

We can express $$ X $$ as : $$ X = \sum S_iV_i{U_i}^T $$

The whole point of performing an SVD is to define : $$ X^+ = \sum {S_i}^{-1}V_i{U_i}^T $$. $$ X^+ $$ is called the pseudo-inverse of a matrix. This pseudo-inverse will allow us to compute the pseudo inverse of the Gram matrix :

$$ \hat{\beta} = (X^TX)^+ X^TY = (VSU^TUSV^T)^+(VSU^T)Y $$

$$ = (VS^2V^T)^+(VSU^T)Y = VS^{-2}SU^TY = VS^{-1}U^TY = X^+Y $$

The estimator has now a different form. This means that we can compute again the bias and the variance of this new estimator.

The bias becomes : $$ E(\hat{\beta}) - {\beta} = X^+E(Y) - {\beta} = X^+(X{\beta}) - {\beta} $$
$$ = (X^+X - I_p){\beta} $$ where $$ X^+X $$. The estimator will be biaised, except if $$ X^+X - I_p = 0 $$ which means $$ Ker(X) = {0} $$.

The variance becomes : $$ Var(\hat{\beta}_n) = (X^TX)^+{\sigma}^2 $$
$$ = (VSU^TUSV^T)^+{\sigma}^2 = (VS^{-2}V^T){\sigma}^2 $$
$$ = \sum \frac {({\sigma}_i)^2} {(S_i)^2} V_i {V_i}^T $$

# Transformations

Let's get back to the simple problem in which we wanted to assess the demand for icecream depending on outside temperature. The model we built looked like this :

$$ y_i = \beta_0 + \beta_1 * x_i + u_i $$

where :
- $$ y_i $$ is the icecream demand on a given day
- $$ \beta_0 $$ a constant parameter
- $$ \beta_1 $$ the parameter that assesses the impact of temperature on icecream demand
- $$ x_i  $$ the average temparature for a given day
- $$ u_i  $$ the residuals

However, the assumption that the model is purely linear is a strong assumption, that is pretty much never met in practice. We can apply some transformations to the model to make it more flexible.

## 1. Log

$$ log(y_i) = \beta_0 + \beta_1 * x_i + u_i $$

This also means that 1% change of $$ y_i $$ : $$  \delta_{y_i} $$ is equal to $$ 100 * \beta_1 * \delta_{x_i} $$ .

## 2. Log-Log

$$ log(y_i) = \beta_0 + \beta_1 * log(x_i) + u_i $$

## 3. Square Root

$$ y_i = \beta_0 + \beta_1 * \sqrt {x_i} + u_i $$

## 4. Quadratic

$$ y_i = \beta_0 + \beta_1 * {X_1}_i + \beta_2 * {X_{2i}}^2 + u_i $$

This implies that :

$$ \frac { \delta_{ {X_1}_i } } { \delta_{ {X_2}_i } } = \beta_1 + 2 * \beta_2 * {X_2}_i  $$

## 5. Non-linear

$$ y_i = \frac {1} {\beta_0 + \beta_1 * {x}_i } + u_i $$

This is a simple example of a non-linear transform.

# Boolean and Categorical Variables 

Up to now, we mostly considered cases in which the variables were continuous (temperature for example). But the variety of data you might deal with might include categorical or boolean variables !

## I. Binary Variables

Let's get back to our icecream demand forecasting problem :

$$ y_i = \beta_0 + \beta_1 * x_i + u_i $$

where :
- $$ y_i $$ is the icecream demand on a given day
- $$ \beta_0 $$ a constant parameter
- $$ \beta_1 $$ the parameter that assesses the impact of temperature on icecream demand
- $$ x_i  $$ the average temparature for a given day
- $$ u_i  $$ the residuals

A binary variable that might be interesting to predict icecream's consumption is the fact that there is public holiday on the day considered or not :

$$ y_i = \beta_0 + \beta_1 * {X_1}_i + \beta_2 * {X_2}_i + u_i $$

where :
- $$ {X_2}_i = 1 $$ is there is public holiday this day
- $$ {X_2}_i = 0 $$ otherwise

Therefore, we can see $$ \beta_2 $$ as :

$$ \beta_2 = E (y_i \mid holiday, {X_1}_i) - E (y_i \mid no-holiday, {X_1}_i )

Public holiday has an impact on icecream demand if $$ \beta_2 $$ is significantly different from 0. This can be seen as a classical hypothesis testing :
- Null hypothesis $$ H_0 : \beta_2 = 0 $$
- and $$ H_0 : \beta_2 ≠ 0 $$

## II. Categorical Variables

We might also have more than 2 categories. For example, regarding our public holiday variable, we'll now be intersted in which state/region of France is on public holiday (there are 3 overall, to avoid have every body sharing the same week of holiday). 

$$ y_i = \beta_0 + \beta_1 * {X_1}_i + \beta_2 * {X_2}_i + \beta_3 * {X_3}_i + \beta_4 * {X_4}_i + u_i $$

where :
- $$ {X_2}_i = 1 $$ is zone A is on holiday
- $$ {X_3}_i = 1 $$ is zone B is on holiday
- $$ {X_4}_i = 1 $$ is zone C is on holiday

The reference case we implicitely consider here is the case in which there is not public holiday. The three terms are here to evaluate a difference with the reference case.

We can run individual or joint tests on the different coefficients of the regression.

## III. Interaction variable

The intuition behind the interaction variable is quite simple. Suppose we want to test the effect of temperature on consumption if zone A is on holiday. 

The problem can be formulated as follows :

$$ y_i = \beta_0 + \beta_1 * {X_1}_i + \beta_2 * {X_2}_i + \theta_2 * {X_2}_i *  {X_1}_i + \beta_3 * {X_3}_i + \beta_4 * {X_4}_i + u_i $$

Here, $$ \theta_2 $$ measures the effect of temperature on icecream consumption when zone A is on holiday. 

We can run student tests on $$ \theta_2 $$ directly.

We  can now enrich our model and add several interaction terms :

$$ y_i = \beta_0 + \beta_1 * {X_1}_i + \beta_2 * {X_2}_i + \theta_2 * {X_2}_i *  {X_1}_i + \beta_3 * {X_3}_i + \theta_3 * {X_3}_i *  {X_1}_i  + \beta_4 * {X_4}_i + \theta_4 * {X_4}_i *  {X_1}_i + u_i $$

If we want to test the difference between the groups, we should make a Fisher test with a null hypothesis $$ H_0 : \theta_2 = 0, \theta_3 = 0, \theta_4 = 0 $$.

# Dealing with heteroscedasticity

Heteroscedasticity might be an issue when conducting hypothesis tests. How can we define heteroscedasticity ? How can we detect it ? How can we overcome this issue. 

## I. What is heteroscedasticity ?

Heteroscedasticity (or heteroskedasticity) refers to the case in which the variablity is unequal across the range of values. 

Recall that in the linear regression framework : $$ y = \beta_0 + \beta_1 X_1 + ... + \beta_k X_k + u $$. The fundamental hypothesis is that : $$ E (u \mid X_1, X_2, ..., X_k) = 0 $$. Under this hypothesis, the OLS estimator is the Best Linear Unbiaised Estimator (BLUE). 

By normality hypothesis, under homoscedasticity, $$ u \sim N(0, \sigma^2) $$ and $$ Var (u \mid X_1, ... X_k) = \sigma^2 $$.

For example, if we try to predict the income in terms of the age of a person :
- in case of homoscedasticity, the variance is constant over the age of the person, i.e 
- in case of heteroscedasticity, the variance is increasing over the age of the person

![image](https://maelfabien.github.io/assets/images/hetero.png)

Under heteroscedasticity, $$ Var (u_i \mid X_i) = {\sigma_i}^2 $$.

## II. Transform the variables in `log`

In most cases, to control for heteroscedasticity, there is an easy trick : transform the variables in log. 

$$ log(y) = \beta_0 + \beta_1 X_1 + ... + \beta_k X_k + u $$

Most of the time, this will do the trick !

For example, in Matlab :

```matlab
y = log(hprice1(:,1));
[n,k] = size(hprice1);

X = [ones(n,1), hprice1(:,[3,4,5])]
beta = inv(X'*X)*X'*y
```

## III. Robust inference to heteroscedasticity 

### Concept

*Idea* : Build a standard error robust to any kind of heteroscedasticity. 

The estimators take the following form :

$$ \hat{\beta_1} = \beta_1 + \frac {\sum_i (X_i - \bar{X})^2 \hat{u_i}^2} { \sum_i (X_i - \bar{X})^2} $$

This estimator is robust to any kind of heteroscedasticity. The variance of the estimator is defined by :

$$ Var( \hat{\beta_1} ) = \frac { \sum_i ( X_i - \bar{X})^2 {\sigma_i}^2 } { { SSR_x }^2 } $$

This requires $$ \beta_1 $$ to be known and $$ u_i $$ too. We can define $$ \hat{u_i} $$ the residual of the estimation. Using White's formula:

$$ \frac { \sum_i (X_i - \bar{X})^2 \hat{u_i}^2 } { { SSR_x }^2 } $$ is robust to any kind of heteroscedasticity. 

Therefore, if $$ r_{ij} $$ is the residual of the regression of $$ X_{j} $$ on the other independant variables, we have :

$$ \hat{Var}( \hat{\beta_j} ) = \frac {\sum_i \hat{r_{ij}}^2 \hat{u_i}^2 } { {SSR_j}^2 } $$

It is sufficient to modify the reference standard error during further tests. 

### Detect heteroscedasticity 

How can we detect heteroscedasticity ? A simple test hypothesis can be used :

$$ H_0 : Var(u \mid X_1, ..., X_k) = \sigma^2 $$

## IV. Linear form residuals

The residuals in case of heteroscedasticity might depend on :
- time, with an index $$ i $$, as we have seen up to now
- other features

In the second case, the residuals should have the following form :

$$ u^2 = \delta_0 + \delta_1 X_1 + ... + \delta_k X_k + V $$

We can test the hypothesis $$ H_0 : \delta_1 = \delta_2 = ... = \delta_k = 0 $$

## V. Heteroscedasticity with a constant shift

This time, the hypothesis is the following :

$$ Var(u \mid x_i) = \sigma^2 h(x_i) $$

Therefore, we might need to modify the residuals :

$$ E( ( \frac {u_i} {\sqrt{h_i} } )^2 ) = \frac {E({u_i}^2)} {h_i} = \frac { \sigma^2 h_i} {h_i} = \sigma^2 $$

Now, let's use the transformed model to fit our regression :

$$ \frac {y_i} {\sqrt{h_i} } = \frac {\beta_0} {\sqrt{h_i} } + \beta_1 \frac { X_{i1} } { \sqrt{h_i} } + ... + \beta_k \frac { X_{ik} } { \sqrt{h_i} } + \frac {u_i} { \sqrt{h_i} } $$

We obtain a weighted least squares problem :

$$ \frac {\sum_i (y_i - b_0 - b_1 X_{i1} - ... - b_k X_{ik})^2 } {h_i} $$

In practice, it is hard to find those weights. For this reson, we usually apply what we call generalized least squares (GLS). 

# Generalized Least Squares

In the previous sections, we highlighted the need for models and estimators that handle heteroscedasticity. We'll introduce Generalized Least Squares, a more general framework. 

Recall that in the OLS framework, the Best Linear Unbiaised Estimator was defined by :

$$ b = (X' X)^{-1} X' Y = \beta + (X'X)^{-1}X' \epsilon $$

We have seen previously that under heteroscedasticity, this estimator was no longer the best. It remained however a Linear Unbiaised Estimator. 

Suppose than we know $$ \Omega $$ the covariance of the errors such that it can be split the following way by Chelesky decomposition : $$ \Omega^{-1} = P'P $$, $$ P $$  a triangular matrix. 

In this case, we can build the GLS estimator by scaling the initial regression problem :

$$ Py = PX \beta + P \epsilon $$

We can define new shifted variables :

$$ Y^{*} = X^{*} \beta + \epsilon^{*} $$

The resulting estimator is therefore modified too :

$$ \hat{\beta} = (X^{'*} X^{*})^{-1} X^{'*} Y^{*} = (X'P'PX)^{-1}X'P'Py = (X' \Omega^{-1} X)^{-1} X' \Omega^{-1} y $$

The variance of the estimator becomes :

$$ Var( \hat{\beta} \mid X*) = \sigma^2(X'*X'*)^{-1} = \sigma^2(X' \Omega^{-1} X)^{-1} $$

## 1. Weighted Least Squares

The Weighted Least Squares is a special case of the GLS framework. 

Under heteroscedasticity, we might face : $$ Var( \epsilon_i \mid X_i) = {\sigma_i}^2 = \sigma^2 w_i $$

In such case, we can specify $$ \Omega^{-1} $$, which has diagnoal elements of the type $$ \frac {1} {w_i} $$

If we replace the next value of $$ \Omega $$ in the GLS estimator :

$$ \hat{\beta} = ( \sum_i w_i X_i X_i' )^{-1} ( \sum_i w_i X_i y_i) $$

The estimator is consistent whatever the weights being used. 

## 2. Feasible Least Squares

Usually, we don't know the covariance of the errors, but it usually depends on some parameters that can be estimated. This is the main idea behind Feasible Least Squares.

$$ \hat{ \omega} = \Omega ( \hat{ \theta} ) $$ 

We can use this estimator in the estimator's formula :

$$ \hat {\hat { \beta } } = (X' \hat{ \Omega^{-1}} X)^{-1} X' \hat{\Omega^{-1}} Y $$

The Github repository of this article can be found [here](https://github.com/maelfabien/Machine_Learning_Tutorials).

> **Conclusion** : So far, we understood the motivation behind trying to find a model and parameters that fit our datas, the concept of linear regression and its applications in Python, the concept of tests, and confidence intervals. 

In the next article, we will be adding some features to our observations and exploring the multi-dimensional linear regression ! Hope you enjoyed the article.
