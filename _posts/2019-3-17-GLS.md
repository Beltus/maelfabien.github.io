---
published: true
title: Generalized Least Squares (GLS)
collection: st
layout: single
author_profile: false
read_time: true
categories: [statistics]
excerpt : "Linear Model"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
    teaser : "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
toc_sticky: true
---

In the previous article, we highlighted the need for models and estimators that handle heteroscedasticity. We'll introduce Generalized Least Squares, a more general framework. 

<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

{% highlight matlab %}
{% endhighlight %}

Recall that in the OLS framework, the Best Linear Unbiaised Estimator was defined by :

$$ b = (X' X)^{-1} X' Y = \beta + (X'X)^{-1}X' \epsilon $$

We have seen previously that under heteroscedasticity, this estimator was no longer the best. It remained however a Linear Unbiaised Estimator. 

# I. The Generalized Least Squares Estimator

Suppose than we know $$ \Omega $$ the covariance of the errors such that it can be split the following way by Chelesky decomposition : $$ \Omega^{-1} = P'P $$, $$ P $$  a triangular matrix. 

In this case, we can build the GLS estimator by scaling the initial regression problem :

$$ Py = PX \beta + P \epsilon $$

We can define new shifted variables :

$$ Y^{*} = X^{*} \beta + \epsilon^{*} $$

The resulting estimator is therefore modified too :

$$ \hat{\beta} = (X^{'*} X^{*})^{-1} X^{'*} Y^{*} = (X'P'PX)^{-1}X'P'Py = (X' \Omega^{-1} X)^{-1} X' \Omega^{-1} y $$

The variance of the estimator becomes :

$$ Var( \hat{\beta} \mid X*) = \sigma^2(X'*X'*)^{-1} = \sigma^2(X' \Omega^{-1} X)^{-1} $$

## 1. Weighted Least Squares

The Weighted Least Squares is a special case of the GLS framework. 

Under heteroscedasticity, we might face : $$ Var( \epsilon_i \mid X_i) = {\sigma_i}^2 = \sigma^2 w_i $$

In such case, we can specify $$ \Omega^{-1} $$, which has diagnoal elements of the type $$ \frac {1} {w_i} $$

If we replace the next value of $$ \Omega $$ in the GLS estimator :

$$ \hat{\beta} = ( \sum_i w_i X_i X_i' )^{-1} ( \sum_i w_i X_i y_i) $$

The estimator is consistent whatever the weights being used. 

## 2. Feasible Least Squares

Usually, we don't know the covariance of the errors, but it usually depends on some parameters that can be estimated. This is the main idea behind Feasible Least Squares.

$$ \hat{ \omega} = \Omega ( \hat{ \theta} ) $$ 

We can use this estimator in the estimator's formula :

$$ \hat {\hat { \beta } } = (X' \hat{ \Omega^{-1}} X)^{-1} X' \hat{\Omega^{-1}} Y $$

> **Conclusion** : I hope you found this article useful. Don't hesitate to drop a comment if you have a question.
