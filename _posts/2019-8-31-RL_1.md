---
published: true
title: Introduction to Reinforcement Learning
collection: rl
layout: single
author_profile: false
read_time: true
categories: [RL]
excerpt : "Advanced AI"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
    teaser: "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
toc_sticky: true
sidebar:
    nav: sidebar-sample
---

I was recently recommended to take a look at David Silver's (from DeepMind) YouTube series on Reinforcement Learning. The whole course (10 videos) can be found [here](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ).

In this series, I am going to :
- summarize the important notions taught in each course
- add examples and code
- illustrate some concepts with schemas
- do a Deep RL project
- go a bit further by exploring Twin Delayed DDPG and other SOTA models

# Introduction to RL

Reinforcement Learning is at the intersection of many fields of science :
- Machine Learning
- Reward System
- Classical/Operant conditioning
- Bounded Rationality
- Operations Research
- Optimal Control

How is RL different from other ML paraddigms ?
- There is no supervisor, only a reward signal
- Feedback is delayed, not instantaneous
- Time matters, since we consider sequences
- The agent's actions affect the data it sees

Where is RL applied ?
- manage an investment portfolio
- make a robot walk
- control a power station
- play games (Atari, Go...)

## The RL learning problem

A **reward** $$ R_t $$ is a feedback value. In indicates how well the agent is doing at step $$ t $$. The job of the agent is to maximize the cumulative reward. 

> Reward Hypothesis : All goals can be described by the maximisation of expected cumulative reward.

Some reward examples :
- give reward to the agent if it defeats the Go champion
- give reward to the agent for each $ that gets in the bank (investment portfolio)
- give reward to the agent if the score of the game increases
- ...

The reward might be at the end of the whole process, or at each step, depending on the type of problem we consider.

## The environment

> This is a **Sequential Decision Making** process. We select actions to maximise the total future reward.

Actions can have long term consequences, and the reward might be delayed. It may be better to sacrifice immediate reward to gain more long-term reward.

- The agent makes an **observation** $$ O_t $$. 
- The agent then takes **action** $$ A_t $$. 
- Based on this action, it gets a **reward** $$ R_t $$.
- There is a new environment, and a new observation is made

It can be represented in the following way :

![image](https://maelfabien.github.io/assets/images/rl_0.png)

The only leverage the agent has on its environment and the reward it gets is the action it takes.

## History and State

> The **history** is the sequence of observations, actions and rewards. It's all the observable variables up to time $$ t $$ : $$ H_t = A_1, O_1, R_1, \cdots, A_t, O_t, R_t $$

What happens next depends on this history. Our aim is simply to find the right action to take based on the history.

We don't always use the whole history. Instea, we use the **state**. It is the information that is used to determine what happens next. It captures all necessary information, and is a function of the history.

$$ S_t = f(H_t) $$

> The **environment state** is what is used inside of the environment to determine what should come next : $$ S_t^e $$. 

If you want to go from A to B, to decide where to turn, you don't need a map of the whole world, but simply things that are relevant around you. This is the idea behind environment state.

The environment state is however not usually visible to the agent. An agent being in an environment state does not mean that the agent gets data from the whole environment around him or captures data from this whole environment. Only the **observations** are sent to the agent.

![image](https://maelfabien.github.io/assets/images/rl_1.png)

> The **agent state** $$ S_t^a $$ is the agent's internal representation. It is all the information used by the agent to take the action. It is a function of the history : $$ S_t^a = f(H_t) $$

![image](https://maelfabien.github.io/assets/images/rl_2.png)

The environment state and the agent state are both **information states**. An information state is a **Markov State**. It contains all information from the history :

> A **state** $$ S_t $$ is a Markov State if and only if $$ P(S_{t+1} \mid S_t) = P(S_{t+1 \mid S_1, ..., S_t) $$

Once a state is known, the history may be thrown away. The current state is a sufficient statistic of the future. The environment state $$ S_t^e $$ is Markov. The history $$ H_t $$ is Markov.
