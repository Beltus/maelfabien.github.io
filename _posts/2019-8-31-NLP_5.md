---
published: true
title: Text classification from few training examples
collection: ml
layout: single
author_profile: false
read_time: true
categories: [machinelearning]
excerpt : "Natural Language Processing"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/lgen_head.png"
    teaser : "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
toc_sticky: true
sidebar:
    nav: sidebar-sample
---

I recently came across a good blog post article published by Nicolas Thiebaut right [here](https://data4thought.com/fewshot_learning_nlp.html). This article addresses the task of classifying texts when we have few training examples.

This kind of problem, known as **Zero-shot learning** (if some classes don't have observations), **One-shot learning** (if we have 1 observation per class),  or **Few-shot learning** (if we have few observations per class), is being studied in the field of computer vision. [Papers](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf) published in this field quite often rely on Siamese Networks. A typical application of such problem would be to build a Face Recognition algorithm. You have 1 or 2 pictures per person, and need to assess who is on the video the camera is filming.

However, in the domain of Natural Language Processing, this problem is by far less studied. It's a shame since there are many applications to few-shot learning in NLP. 

For example : 

![image](https://maelfabien.github.io/assets/images/lgen_6.png)
