---
published: true
title: Word2Vec Embedding
collection: st
layout: single
author_profile: tuto
read_time: true
categories: [NLP]
excerpt : "Natural Language Processing"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
    teaser : "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
toc_sticky: true
---

In order to analyze text and run algorithms on it, we need to embed the text. The notion of embedding simply means that we'll conver the input text into a set of numerical vectors that can be used into algorithms. In this article, we'll focus on Word2Vec, a state of the art embedding method

For the sake of clarity, we'll call a document a simple text, and each document is made of words, which we'll call terms.

<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

{% highlight matlab %}
{% endhighlight %}

# I. What is Bag-Of-Words ?

## 1. Term Frequency

When we use Bag-Of-Words approaches, we apply a simple *word embedding* technique. Technically speaking, we take our whole corpus that has been preprocessed, and create a giant matrix :
- the columns correspond to all the vocabulary that has ever been used with all the documents we have at our disposal
- the lines correspond to each of the document
- the value at each position corresponds to the number of occurence of a given token within a given document

Bag-Of-Words (BOW) can be illustrated the following way :

![image](https://maelfabien.github.io/assets/images/nlp_2.png)

The number we fill the matrix with are simply the raw count of the tokens in each document. This is called the ***term frequency*** (TF) approach.

$$ tf_{t,d} = f_{t,d} $$

where :
- the term or token is denoted $$ t $$
- the document is denoted $$ d $$
- and $$ f $$ is the raw count

This approach is infact only half of the story ! What are the limitations implied by this model ?
- the more frequent a word, the more importance we attach to it within each document which is logic. However, this can be problematic since common words, like cat or dog in our example, do not bring much information about the document. In other words, a word that appear

## 2. Term Frequency Inverse Document Frequency (TF-IDF)

For the reasons mentioned above, the TF-IDF methods were quite popular for a long time, before more advanced techniques like Word2Vec or Universal Sentence Encoder.

In TF-IDF, instead of filling the BOW matrix with the raw count, we simply fill it with the term frequency multiplied by the inverse document frequency. It is intended to reflect how important a word is to a document in a collection or corpus.

$$ tf_idf{t,d} = f_{t,d} \times \frac {1} { f_{t} }  $$

# II. Implementation in Python

Let's now implement this in Python. The first step is to import NLTK library and the useful packages :

```python
import numpy as np
import pandas as pd
import re
from sklearn.feature_extraction.text import CountVectorizer
from nltk import wordpunct_tokenize, WordNetLemmatizer, sent_tokenize, pos_tag
from nltk.corpus import stopwords as sw, wordnet as wn
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import string 
```

## 1. Preprocessing per document within corpus

We can define the preprocessing pipeline that will process each document as a single entity and apply the preprocessing on it :

```python
def preprocess(document, max_features = 150, max_sentence_len = 300):
    """
    Returns a normalized, lemmatized list of tokens 
    from a list of document, applying word/punctuation
    tokenization, and finally part of speech tagging. It uses the part of
    speech tags to look up the lemma in WordNet, and returns the lowercase  
    version of all the words, removing stopwords and punctuation.
    """

    def lemmatize(token, tag):
        """
        Converts the tag to a WordNet POS tag, then uses that   
        tag to perform an accurate WordNet lemmatization.
        """
        tag = {
            'N': wn.NOUN,
            'V': wn.VERB,
            'R': wn.ADV,
            'J': wn.ADJ
        }.get(tag[0], wn.NOUN)

    return WordNetLemmatizer().lemmatize(token, tag)

    def vectorize(doc, max_features, max_sentence_len):
        """
        Converts a document into a sequence of indices of length max_sentence_len retaining only max_features unique words
        """
        tokenizer = Tokenizer(num_words=max_features)   
        tokenizer.fit_on_texts(doc)
        doc = tokenizer.texts_to_sequences(doc)
        doc_pad = pad_sequences(doc, padding = 'pre', truncating = 'pre', maxlen = max_sentence_len)
        return np.squeeze(doc_pad), tokenizer.word_index

    cleaned_document = []
    vocab = []

    # For each document inside the corpus
    for sent in document:
    
        sent = re.sub(r"[^A-Za-z0-9^,!.\/'+-=]", " ", sent)
        sent = re.sub(r"what's", "what is ", sent)
        sent = re.sub(r"\'", " ", sent)
        sent = re.sub(r"@", " ", sent)
        sent = re.sub(r"\'ve", " have ", sent)
        sent = re.sub(r"can't", "cannot ", sent)
        sent = re.sub(r"n't", " not ", sent)
        sent = re.sub(r"i'm", "i am ", sent)
        sent = re.sub(r"\'re", " are ", sent)
        sent = re.sub(r"\'d", " would ", sent)
        sent = re.sub(r"\'ll", " will ", sent)
        sent = re.sub(r"(\d+)(k)", r"\g<1>000", sent)
        sent = sent.replace("\n", " ")
    
        lemmatized_tokens = []

        # Break the sentence into part of speech tagged tokens
        for token, tag in pos_tag(wordpunct_tokenize(sent)):

            # Apply preprocessing to the tokens
            token = token.lower()
            token = token.strip()
            token = token.strip('_')
            token = token.strip('*')
    
            # If punctuation ignore token and continue
            if all(char in set(string.punctuation) for char in token): #token in set(sw.words('english')) or 
                continue

            # Lemmatize the token
            lemma = lemmatize(token, tag)
            lemmatized_tokens.append(lemma)
            vocab.append(lemma)

        cleaned_document.append(lemmatized_tokens)

    vocab = sorted(list(set(vocab)))

    return cleaned_document, vocab
```

Note that this pipeline is only an example that happened to suit my needs on several NLP projects. It has many limitations, including the face that it only handles english vocabulary.

We will work with some data from the South Park series. The data set is made of all the conversations of all the characters in South Park. The data can be downloaded [here]( https://github.com/BobAdamsEE/SouthParkData). We'll focus here on the first 1000 rows of the data set.

```python
df = pd.read_csv('All-Seasons.csv')['Line'][:1000]
df.head()
```

```
0           You guys, you guys! Chef is going away. \n
1                          Going away? For how long?\n
2                                           Forever.\n
3                                    I'm sorry boys.\n
4    Chef said he's been bored, so he joining a gro...
...
```

Let's now apply our preprocessing to the data set :

```python
df, vocab = preprocess(list(df))
```

The new data set will now look like this :

```python
[['you', 'guy', 'you', 'guy', 'chef', 'be', 'go', 'away'],
['go', 'away', 'for', 'how', 'long'],
['forever'],
...
```

And the vocabulary, which has size 1569 here, looks like this :

```
...
'aaaah',
'able',
'aboat',
'aborigine',
'abort',
'about',
'academy',
'acceptance',
'acid',
'ack',
'across',
'act'
...
```

Let us now define the BOW function ! Note that the following implementation is by far not optimized.

```python
def generate_bow(allsentences):    
    # Define the BOW matrix
    bag_vector = np.zeros((len(allsentences), len(vocab)))
    # For each sentence
    for j in range(len(allsentences)):
        # For each word within the sentence
        for w in allsentences[j]:
            # For each word within the vocabulary
            for i,word in enumerate(vocab):
                # If the word is in vocabulary, add 1 in position
                if word == w: 
                    bag_vector[j,i] += 1
    return bag_vector
```

We can then apply the BOW function to the cleaned data :

```python
bow = generate_bow(df)
```

It generates the whole matrix for the 1000 rows in 1.42s.

```python
array([[0., 0., 0., ..., 0., 0., 0.],
[0., 0., 0., ..., 0., 0., 0.],
[0., 0., 0., ..., 0., 0., 0.],
...
```

## 2. The same, in Sk-learn

The is a self-defined function. As you might guess, there is a Sk-learn way to do this :)

You might need to modify a bit the preprocessing function. Indeed, the only thing you'll want to modify is when you append the lematized tokens to the `clean_document` variable :

```python
cleaned_document.append(' '.join(lemmatized_tokens))
```

After which the application in Sk-learn is straightforward :

```python
df = preprocess(list(df))
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(df).toarray()
```

The count vectorizer runs in 50ms.

## 3. Preprocessing per sentence 

If your aim is to do an embedding per sentence, without taking into account the structure of the documents within the corpus, then, this pipeline might be more appropriate :

```python
def preprocess(document, max_features = 150, max_sentence_len = 300):
    """
    Returns a normalized, lemmatized list of tokens from a document by
    applying segmentation (breaking into sentences), then word/punctuation
    tokenization, and finally part of speech tagging. It uses the part of
    speech tags to look up the lemma in WordNet, and returns the lowercase
    version of all the words, removing stopwords and punctuation.
    """

    def lemmatize(token, tag):
        """
        Converts the tag to a WordNet POS tag, then uses that
        tag to perform an accurate WordNet lemmatization.
        """
        tag = {
            'N': wn.NOUN,
            'V': wn.VERB,
            'R': wn.ADV,
            'J': wn.ADJ
        }.get(tag[0], wn.NOUN)

    return WordNetLemmatizer().lemmatize(token, tag)

    def vectorize(doc, max_features, max_sentence_len):
        """
        Converts a document into a sequence of indices of length max_sentence_len retaining only max_features unique words
        """
        tokenizer = Tokenizer(num_words=max_features)
        tokenizer.fit_on_texts(doc)
        doc = tokenizer.texts_to_sequences(doc)
        doc_pad = pad_sequences(doc, padding = 'pre', truncating = 'pre', maxlen = max_sentence_len)
        return np.squeeze(doc_pad), tokenizer.word_index

    cleaned_document = []
    vocab = []

    # Clean the text using a few regular expressions
    document = re.sub(r"[^A-Za-z0-9^,!.\/'+-=]", " ", document)
    document = re.sub(r"what's", "what is ", document)
    document = re.sub(r"\'", " ", document)
    document = re.sub(r"@", " ", document)
    document = re.sub(r"\'ve", " have ", document)
    document = re.sub(r"can't", "cannot ", document)
    document = re.sub(r"n't", " not ", document)
    document = re.sub(r"i'm", "i am ", document)
    document = re.sub(r"\'re", " are ", document)
    document = re.sub(r"\'d", " would ", document)
    document = re.sub(r"\'ll", " will ", document)
    document = re.sub(r"(\d+)(k)", r"\g<1>000", document)
    document = document.replace("\n", " ")


    # Break the document into sentences
    for sent in sent_tokenize(document):
        lemmatized_tokens = []

        # Break the sentence into part of speech tagged tokens
        for token, tag in pos_tag(wordpunct_tokenize(sent)):

            # Apply preprocessing to the tokens
            token = token.lower()
            token = token.strip()
            token = token.strip('_')
            token = token.strip('*')

            # If punctuation ignore token and continue
            if all(char in set(string.punctuation) for char in token): #token in set(sw.words('english')) or 
                continue

            # Lemmatize the token
            lemma = lemmatize(token, tag)
            lemmatized_tokens.append(lemma)
            vocab.append(lemma)

        cleaned_document.append(lemmatized_tokens)

    vocab = sorted(list(set(vocab)))
    return cleaned_document, vocab
```

And apply this function on a string version of the whole corpus :

```python
df, vocab = preprocess(str(list(df)))
```

# III. Limits of BOW methods

The reason why BOW methods are not so popular these days are the following :
- the voculabulary size might get very, very (very) large, and handling a sparse matrix with over 100'000 features is not so cool. If you want to control it, you should set a maximum document length, or a maximum vocabulary length. Both imply large biases.
- the order of the words in the sentence does not matter, which is a major limitation

For example, the sentences : "The cat’s food was eaten by the dog in a few seconds." does not have the same meaning at all than "The dog's food was eaten by the cat in a few seconds.". To overcome the dimension's issue of BOW, it is quite frequent to apply Principal Component Analysis on top of the BOW matrix. 

In the next article, we'll see more evolved techniques like Word2Vec which perform much better and are currently close to state of the art.

> **Conclusion** : I hope this quick introduction to Bag-Of-Words in NLP was helpful. Don't hesitate to drop a comment if you have a comment.
